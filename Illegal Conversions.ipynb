{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-23 09:17:58.752173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print (str(datetime.now()) )\n",
    "print ()\n",
    "\n",
    "#import imblearn\n",
    "#from imblearn import under_sampling, over_sampling\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#path = \"C:\\\\Users\\\\DOBUSER\\\\Documents\\\\RM Tim Case Study\\\\Complaints\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "def CleanVioData (dfa, start_date):\n",
    "    dfa['Date Received'] = pd.to_datetime(dfa['Date Received'])\n",
    "    #dfa['InspectionDate'] = pd.to_datetime(dfa['InspectionDate'])\n",
    "    dfa = dfa.rename(columns={'Vacate issued': 'Outcome'})\n",
    "\n",
    "\n",
    "    \n",
    "    #from dateutil import parser\n",
    "    #start_date = parser.parse(start_date)\n",
    "    #start_date = start_date.date()\n",
    "\n",
    "    mask = (dfa['Date Received'] >= start_date)\n",
    "    print (\"len before date drop\", len(dfa) )\n",
    "    dfa = dfa.loc[mask].reset_index(drop=True)\n",
    "    print (\"len after date drop\", len(dfa) )\n",
    "\n",
    "    #dfa = dfa.rename(columns={'D_DATE': 'InspectionDate'})\n",
    "\n",
    "    print (\"number of nans on the vios\")\n",
    "    print (dfa[\"Outcome\"].isnull().sum() )\n",
    "\n",
    "    df2 = dfa.copy()\n",
    "    df2[\"Count\"] = 1\n",
    "    \n",
    "    df2 = df2[[\"Outcome\", \"Count\"]]\n",
    "    df2 = df2.groupby(['Outcome']).sum()\n",
    "    df2 = df2.add_suffix('').reset_index()\n",
    "    print (df2)\n",
    "    \n",
    "    percent = df2[\"Count\"][0]/ float(df2[\"Count\"][1])\n",
    "    print (\"Percent minority class:\", percent)\n",
    "\n",
    "    #df2 = df2[[\"Source of Complaint Description\", \"Count\"]]\n",
    "    #df2G = df2.groupby(['Source of Complaint Description']).sum()\n",
    "    #df2G = df2G.add_suffix('').reset_index()\n",
    "    #df2G = df2G.sort_values(by = 'Count', ascending=False).reset_index(drop=True)\n",
    "    #print (\"percent dob\" )\n",
    "    #percent = df2G[\"Count\"][1]/float(df2G[\"Count\"].sum() )\n",
    "    #print (percent)\n",
    "    #print (\"percent citizen\")\n",
    "    #percent = df2G[\"Count\"][0]/float(df2G[\"Count\"].sum() )\n",
    "    #print (percent)\n",
    "    \n",
    "    return dfa\n",
    "\n",
    "\n",
    "def CleanOpenData (dfb):\n",
    "    print (\"Length of open data\", len(dfb))\n",
    "    #df2 = dfb.copy()\n",
    "    #print (\"Drop registration id\")\n",
    "    #dfb = dfb.drop('RegistrationID', 1)\n",
    "    #df2[\"Count\"] = 1\n",
    "    #df2 = df2[[\"Source of Complaint Description\", \"Count\"]]\n",
    "    #df2G = df2.groupby(['Source of Complaint Description']).sum()\n",
    "    #df2G = df2G.add_suffix('').reset_index()\n",
    "    #df2G = df2G.sort_values(by = 'Count', ascending=False).reset_index(drop=True)\n",
    "    #print (\"percent dob\")\n",
    "    #percent = df2G[\"Count\"][1]/float(df2G[\"Count\"].sum() )\n",
    "    #print (percent)\n",
    "    #print (\"percent citizen\")\n",
    "    #percent = df2G[\"Count\"][0]/float(df2G[\"Count\"].sum() )\n",
    "    #print (percent)\n",
    "    \n",
    "    return dfb\n",
    "\n",
    "\n",
    "def MergeVioOpen(dfa, dfb):\n",
    "    frames = [dfa, dfb]\n",
    "    df = pd.concat(frames)\n",
    "\n",
    "    cols = df.columns\n",
    "\n",
    "    for c in cols:\n",
    "        s = c.replace(\" \", \"\")\n",
    "        #print (c)\n",
    "        df = df.rename(columns={c: s})\n",
    "\n",
    "    df[\"Outcome\"] = df[\"Outcome\"].fillna(\"Open\")\n",
    "    print (len(df))\n",
    "    #df[\"SourceofComplaintDescription\"] = df[\"SourceofComplaintDescription\"].astype(str)\n",
    "    #remove dob generated complaints, will bias results\n",
    "    #df = df[df[\"SourceofComplaintDescription\"] != \"DOB\"].reset_index(drop=True)\n",
    "    #print (len(df))\n",
    "    print (\"rename BIN\")\n",
    "    df = df.rename(columns={'BINNumber': 'BIN'})\n",
    "    df[\"BIN\"] = df[\"BIN\"].astype(str)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def CountClassesAndClean(df):\n",
    "    df[\"Outcome\"] = df[\"Outcome\"].astype(str).map(str.strip)\n",
    "    dfcount = df[df[\"Outcome\"] == \"Vacate Not Issued\"]\n",
    "    print (\"No vios\")\n",
    "    print (len(dfcount) )\n",
    "    dfcount = df[df[\"Outcome\"] == \"Vacate Issued\"]\n",
    "    print (\"Vios\")\n",
    "    print (len(dfcount) )\n",
    "    dfcount = df[df[\"Outcome\"] == \"Open\"]\n",
    "    print (\"Open\")\n",
    "    print (len(dfcount) )\n",
    "\n",
    "    return df\n",
    "\n",
    "def DateSortAscending(df):\n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "    df = df.sort_values(by = 'DateReceived', ascending=True).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def OutcomeConvert(f):\n",
    "    outcomes = set(f[\"Outcome\"].tolist())\n",
    "    print (outcomes)\n",
    "    print (\"len of before drop open f\", len(f))\n",
    "    f = f[f[\"Outcome\"] != \"Open\"].reset_index(drop=True)\n",
    "    print (\"len of f after drop open\", len(f))\n",
    "    \n",
    "    f['Outcome'] = f['Outcome'].str.replace('Vacate Not Issued', '0')\n",
    "    #f['Outcome'] = f['Outcome'].str.replace('Open', '0')\n",
    "    f['Outcome'] = f['Outcome'].str.replace('Vacate Issued', '1')\n",
    "    f['Outcome'] = f['Outcome'].astype(int)\n",
    "    return f\n",
    "\n",
    "\n",
    "def CleanBuildClass(df):\n",
    "    print (\"creating building class 2, non aggregated\")\n",
    "    df[\"DOFBuildingClassification\"] = df[\"DOFBuildingClassification\"].astype(str).map(str.strip)\n",
    "    df[\"BldgClass\"] = df[\"DOFBuildingClassification\"].str.split(\":\").str[0]\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('1 FAMILY DWELLINGS', '1-2_ FAMILY DWELLINGS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('2 FAMILY DWELLINGS', '1-2_ FAMILY DWELLINGS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('1-2_ FAMILY DWELLINGS', '1-2 FAMILY DWELLINGS')   \n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS ASYLUM, HOME', 'MISCELLANEOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS STORE BUILDINGS', 'MISCELLANEOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS TRANSPORTATION FACILITY', 'MISCELLANEOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS TRANSPORTATION FACILITY', 'MISCELLANEOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('TENNIS COURTS', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MARINAS/YACHT CLUBS', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISC. OUTDOOR RECREATION FACILITY', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('OUTDOOR POOLS', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PARKS RECREATION FACILITY', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PLAYGROUNDS', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('STADIUMS RACE TRACKS BASEBALL FD', 'SPORTS FACILITY')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('DETENTION HSE FOR WAYWARD GIRL', 'CHILDREN RELATED')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('HOMES FOR INDIGENT KIDS/AGED/HOMELESS', 'CHILDREN RELATED')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('ORPHANAGES', 'CHILDREN RELATED')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CHILDREN RELATEDS', 'CHILDREN RELATED')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CHURCH, SYNAGOGUE, CHAPEL', 'RELIGIOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PARSONAGE, RECTORY', 'RELIGIOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CONVENT', 'RELIGIOUS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('&', '_')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('/', '_')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('-', '_')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace(' ', '_')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('___', '_')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('__', '_')\n",
    "    pd.set_option('chained_assignment', None)\n",
    "    bldClass = sorted(list(set(df[\"BldgClass\"].tolist())))\n",
    "    print (bldClass)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def BuildingClassAggregate(df):\n",
    "    df[\"DOFBuildingClassification\"] = df[\"DOFBuildingClassification\"].astype(str).map(str.strip)\n",
    "    df[\"BldgClass\"] = df[\"DOFBuildingClassification\"].str.split(\":\").str[0]\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('1 FAMILY DWELLINGS', '1_2FAMILY_DWELLINGS')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('2 FAMILY DWELLINGS', '1_2FAMILY_DWELLINGS')\n",
    "    #df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('1 FAMILY DWELLINGS', 'Residential')\n",
    "    #df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('2 FAMILY DWELLINGS', 'Residential')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('WALK-UP APTS', 'Residential')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('ELEVATOR APTS', 'Residential')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CONDOMINIUMS', 'Residential')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CONDOMINIUMS', 'Residential')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS STORE BUILDINGS', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('STORE BUILDING', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('GARAGES & GAS STATIONS', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('OFFICE BUILDINGS', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('WAREHOUSES', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('FACTORY/INDUSTRIAL BLDGS', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('HOTELS', 'Commercial')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('RESIDENCE-MULTI USE', 'Multi_use')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('STORES WITH APARTMENTS ABOVE', 'Multi_use')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('SINGLE/MULT RES W/STORES/OFFICES', 'Multi_use')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS TRANSPORTATION FACILITY', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS ASYLUM, HOME', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('VACANT LAND', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CHURCH, SYNAGOGUE, CHAPEL', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PARSONAGE, RECTORY', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('EDUCATIONAL STRUCTURES', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('HOSPITALS & HEALTH', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PLACES OF PUBLIC ASSEMBLY', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('DINERS FRANCHISED TYPE STAND', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PLAYGROUNDS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PIERS DOCKS  BULKHEADS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('SHOPPING CENTER W/PARKING FACILITY', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('PARKS RECREATION FACILITY', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('AIRPORT AIRFIELD  TERMINALS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISSION HOUSE NON-RESIDENTIAL', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('STADIUMS RACE TRACKS BASEBALL FD', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('OUTDOOR POOLS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISC. OUTDOOR RECREATION FACILITY', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('CONVENT', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MARINAS/YACHT CLUBS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('LOFT BUILDINGS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('HOMES FOR INDIGENT KIDS/AGED/HOMELESS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('GOVERNMENT INSTALLATIONS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('FUNERAL HOME', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('DETENTION HSE FOR WAYWARD GIRLS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('DEPARTMENT STORES', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('THEATRES', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('MISCELLANEOUS', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('ORPHANAGES', 'Other')\n",
    "    df[\"BldgClass\"] = df[\"BldgClass\"].str.replace('nan', 'Other')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print (\"DONE\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_block(x):\n",
    "    #Creates median income categorical variable\n",
    "    if len(x) == 1:\n",
    "        return \"0000\" + x\n",
    "    if len(x) == 2:\n",
    "        return \"000\" + x\n",
    "    if len(x) == 3:\n",
    "        return \"00\" + x\n",
    "    if len(x) == 4:\n",
    "        return \"0\" + x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def convert_lot(x):\n",
    "    #Creates median income categorical variable\n",
    "    if len(x) == 1:\n",
    "        return \"000\" + x\n",
    "    if len(x) == 2:\n",
    "        return \"00\" + x\n",
    "    if len(x) == 3:\n",
    "        return \"0\" + x\n",
    "    else:\n",
    "        return x\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS DATA ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-23 09:17:58.791553\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\ndef BuildArea(x):\\n    if x == 0:\\n        return \"nan\"\\n    if (x!=0 and x<=2000):\\n        return \"<2000\"\\n    if (x>2000 and x<=4000):\\n        return \"2000_4000\"\\n    if (x>4000 and x<=6000):\\n        return \"4000_6000\"\\n    if (x>6000 and x<=8000):\\n        return \"6000_8000\"\\n    if (x>8000 and x<=10000):\\n        return \"8000_10000\"\\n    if (x>10000 and x<=20000):\\n        return \"10000_20000\"\\n    if (x>20000):\\n        return \">20000\"\\n        \\ndef BuildArea(x):\\n    if x == 0:\\n        return \"nan\"\\n    if (x!=0 and x<=1000):\\n        return \"<1000\"\\n    if (x>1000 and x<=1500):\\n        return \"1000_1500\"\\n    if (x>1500 and x<=2000):\\n        return \"1500_2000\"\\n    if (x>2000 and x<=2500):\\n        return \"2000_2500\"\\n    if (x>2500 and x<=3000):\\n        return \"2500_3000\"\\n    if (x>3500 and x<=4000):\\n        return \"3500_4000\"    \\n    if (x>4000 and x<=4500):\\n        return \"4000_4500\"  \\n    if (x>4500 and x<=5000):\\n        return \"4500_5000\"  \\n    if (x>5000 and x<=10000):\\n        return \"5000_10000\"\\n    if (x>10000 and x<=20000):\\n        return \"10000_20000\"\\n    if (x>20000):\\n        return \">20000\"\\n\\ndef BuildArea(x):\\n    if x == 0:\\n        return \"nan\"\\n    if (x!=0 and x<=5000):\\n        return \"<5000\"\\n    if (x>5000 and x<=10000):\\n        return \"5000_10000\"\\n    if (x>10000 and x<=15000):\\n        return \"10000_15000\"\\n    if (x>15000 and x<=20000):\\n        return \"15000_20000\"\\n    if (x>20000 and x<=25000):\\n        return \"20000_25000\"\\n    if (x>25000 and x<=30000):\\n        return \"25000_30000\"\\n    if (x>35000 and x<=40000):\\n        return \"35000_40000\"    \\n    if (x>40000 and x<=45000):\\n        return \"40000_45000\"  \\n    if (x>45000 and x<=50000):\\n        return \"45000_50000\"  \\n    if (x>50000 ):\\n        return \"x>50000\"\\n        \\ndef BuildArea(x):\\n    if x == 0:\\n        return \"nan\"\\n    if (x!=0 and x<=10000):\\n        return \"Small\"\\n    if (x>10000 and x<=50000):\\n        return \"Medium\"\\n    if (x>50000 ):\\n        return \"Large\"\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (str(datetime.now()) )\n",
    "print ()\n",
    "\n",
    "def HPDandHPDNA(df, hpd):\n",
    "    hpd[\"BIN\"] = hpd[\"BIN\"].astype(str)\n",
    "    hpdbin = hpd[[\"BIN\"]]\n",
    "    hpdbin[\"HPD\"] = 1\n",
    "    print (hpdbin[\"BIN\"][0])\n",
    "    hpdbin = hpdbin.set_index('BIN')['HPD'].to_dict()\n",
    "    \n",
    "    df[\"HPD\"] = df[\"BIN\"].map(hpdbin)\n",
    "    df[\"HPD\"] = df[\"HPD\"].fillna(0)\n",
    "\n",
    "    print (\"sum of hpd\",df[\"HPD\"].sum() )\n",
    "\n",
    "    print (\"percent hpd\", df[\"HPD\"].sum()/len(df))\n",
    "\n",
    "    hpdna = hpd[[\"BIN\", \"HPD Classifications\"]]\n",
    "    hpdna[\"HPD Classifications\"] = hpdna[\"HPD Classifications\"].astype(str)\n",
    "    #classtypes = set(hpd[\"HPD Classifications\"].tolist() )\n",
    "    hpdna = hpdna[hpdna[\"HPD Classifications\"] == \"NOT AVAILABLE\"]\n",
    "    hpdna[\"HPD Classifications\"] = 1\n",
    "    hpdna[\"HPD Classifications\"] = hpdna[\"HPD Classifications\"].astype(int)\n",
    "    hpdna = hpdna.set_index('BIN')['HPD Classifications'].to_dict()\n",
    "    df[\"HPDNA\"] = df[\"BIN\"].map(hpdna)\n",
    "    df[\"HPDNA\"] = df[\"HPDNA\"].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def YearBuildArea(df, dfb):\n",
    "    dfb[\"BIN\"] = dfb[\"BIN\"].astype(str)\n",
    "    yearbuilt = dfb[[\"BIN\", \"YearBuilt\"]]\n",
    "    yearbuilt = yearbuilt.set_index('BIN')['YearBuilt'].to_dict()\n",
    "    df[\"YearBuilt\"] = df[\"BIN\"].map(yearbuilt)\n",
    "    df[\"YearBuilt\"] = df[\"YearBuilt\"].fillna(0)\n",
    "\n",
    "    buildingarea = dfb[[\"BIN\", \"BuildingArea\"]]\n",
    "    buildingarea = buildingarea.set_index('BIN')['BuildingArea'].to_dict()\n",
    "    df[\"BuildingArea\"] = df[\"BIN\"].map(buildingarea)\n",
    "    df[\"BuildingArea\"] = df[\"BuildingArea\"].fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def MultipleDwellingOwner(df, dfm):\n",
    "    dfm[\"BIN\"] = dfm[\"BIN\"].astype(str)\n",
    "    dfm[\"BIN\"] = dfm[\"BIN\"].str.split(\".\").str[0]\n",
    "\n",
    "    dfm = dfm[[\"BIN\", \"Ownership Profile\", \"Owner Type\"]]\n",
    "\n",
    "    cols = dfm.columns\n",
    "    for c in cols:    \n",
    "        s = c.replace(' ', '')\n",
    "        dfm = dfm.rename(columns={c: s})\n",
    "    print (cols)\n",
    "\n",
    "    ownDic = dfm.set_index('BIN')['OwnershipProfile'].to_dict()\n",
    "    df[\"Owner\"] = df[\"BIN\"].map(ownDic)\n",
    "\n",
    "    #owner = df[[\"SWOissued\", \"Owner\"]]\n",
    "    #owner[\"Count\"] = 1\n",
    "    #owner = owner.groupby(['Owner']).sum()\n",
    "    #owner = owner.add_suffix('').reset_index()\n",
    "    #owner = owner.sort_values(by = 'SWOissued', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    #print \n",
    "    #print (owner)\n",
    "\n",
    "    ownType = dfm.set_index('BIN')['OwnerType'].to_dict()\n",
    "    df[\"OwnerType\"] = df[\"BIN\"].map(ownType)\n",
    "\n",
    "    dfm[\"MD\"] = 1\n",
    "    mdDic = dfm.set_index('BIN')[\"MD\"].to_dict()\n",
    "    df[\"MD\"] = df[\"BIN\"].map(mdDic)\n",
    "    df[\"MD\"] = df[\"MD\"].fillna(0)\n",
    "    \n",
    "    print (\"len of df\", len(df))\n",
    "    print (\"number of MDs\", df[\"MD\"].sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def CensusData(df, dc):\n",
    "    dc = dc.rename(columns={\"Bin\": \"BIN\"})\n",
    "    dc[\"BIN\"] = dc[\"BIN\"].astype(str)\n",
    "    dc['MedianIncome'] = dc['MedianIncome'].fillna(0)  \n",
    "    dc['MedianIncome'] = dc['MedianIncome'].astype(str)\n",
    "    dc['MedianIncome'] = dc['MedianIncome'].str.split(\".\").str[0]\n",
    "    dc['MedianIncome'] = dc['MedianIncome'].str.replace('[^\\w\\s]','')\n",
    "    dc['MedianIncome'] = dc['MedianIncome'].astype(int)\n",
    "    incomeDic = dc.set_index('BIN')['MedianIncome'].to_dict()\n",
    "\n",
    "    df[\"MedianIncome\"] = df[\"BIN\"].map(incomeDic)\n",
    "\n",
    "    print (\"income null\")\n",
    "    print (df[\"MedianIncome\"].isnull().sum() )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def highrise_cat(x):\n",
    "    if x >= 7:\n",
    "        return 1\n",
    "    if x < 7:\n",
    "        return 0\n",
    "    \n",
    "def YearBuilt(x):\n",
    "    if x == 0:\n",
    "        return \"nan\"\n",
    "    if (x!=0 and x < 1900):\n",
    "        return \"pre_1900\"\n",
    "    if (x >= 1900 and x < 1916):\n",
    "        return \"1900_1915\"\n",
    "    if (x >= 1916 and x < 1938):\n",
    "        return \"1916_1937\"   \n",
    "    if (x >= 1938 and x < 1968):\n",
    "        return \"1938_1967\"   \n",
    "    if (x >= 1968 and x < 2008):\n",
    "        return \"1968_2007\" \n",
    "    if (x >= 2008 and x < 2021):\n",
    "        return \"2008_2021\" \n",
    "    \n",
    "\n",
    "def BuildArea(x):\n",
    "    if x == 0:\n",
    "        return \"nan\"\n",
    "    if (x!=0 and x<=3000):\n",
    "        return \"<3000\"\n",
    "    if (x>3000 and x<=6000):\n",
    "        return \"3000_6000\"\n",
    "    if (x>6000 and x<=9000):\n",
    "        return \"6000_9000\"\n",
    "    if (x>9000 and x<=12000):\n",
    "        return \"9000_12000\"\n",
    "    if (x>12000 and x<=20000):\n",
    "        return \"12000_20000\"\n",
    "    if (x>20000):\n",
    "        return \">20000\"\n",
    "        \n",
    "\n",
    "    \n",
    "def income_cat(x):\n",
    "    \"\"\"Creates median income categorical variable\"\"\"\n",
    "    if x == 0:\n",
    "        return \"NAN\"\n",
    "    if x > 0 and x <= 40000:\n",
    "        return \"Low\"\n",
    "    if x > 40000 and x <= 60000:\n",
    "        return \"Medium_Low\"\n",
    "    if x > 60000 and x <= 90000:\n",
    "        return \"Medium_High\"\n",
    "    if x > 90000:\n",
    "        return \"High\"\n",
    "        \n",
    "\n",
    "\"\"\" \n",
    "def BuildArea(x):\n",
    "    if x == 0:\n",
    "        return \"nan\"\n",
    "    if (x!=0 and x<=2000):\n",
    "        return \"<2000\"\n",
    "    if (x>2000 and x<=4000):\n",
    "        return \"2000_4000\"\n",
    "    if (x>4000 and x<=6000):\n",
    "        return \"4000_6000\"\n",
    "    if (x>6000 and x<=8000):\n",
    "        return \"6000_8000\"\n",
    "    if (x>8000 and x<=10000):\n",
    "        return \"8000_10000\"\n",
    "    if (x>10000 and x<=20000):\n",
    "        return \"10000_20000\"\n",
    "    if (x>20000):\n",
    "        return \">20000\"\n",
    "        \n",
    "def BuildArea(x):\n",
    "    if x == 0:\n",
    "        return \"nan\"\n",
    "    if (x!=0 and x<=1000):\n",
    "        return \"<1000\"\n",
    "    if (x>1000 and x<=1500):\n",
    "        return \"1000_1500\"\n",
    "    if (x>1500 and x<=2000):\n",
    "        return \"1500_2000\"\n",
    "    if (x>2000 and x<=2500):\n",
    "        return \"2000_2500\"\n",
    "    if (x>2500 and x<=3000):\n",
    "        return \"2500_3000\"\n",
    "    if (x>3500 and x<=4000):\n",
    "        return \"3500_4000\"    \n",
    "    if (x>4000 and x<=4500):\n",
    "        return \"4000_4500\"  \n",
    "    if (x>4500 and x<=5000):\n",
    "        return \"4500_5000\"  \n",
    "    if (x>5000 and x<=10000):\n",
    "        return \"5000_10000\"\n",
    "    if (x>10000 and x<=20000):\n",
    "        return \"10000_20000\"\n",
    "    if (x>20000):\n",
    "        return \">20000\"\n",
    "\n",
    "def BuildArea(x):\n",
    "    if x == 0:\n",
    "        return \"nan\"\n",
    "    if (x!=0 and x<=5000):\n",
    "        return \"<5000\"\n",
    "    if (x>5000 and x<=10000):\n",
    "        return \"5000_10000\"\n",
    "    if (x>10000 and x<=15000):\n",
    "        return \"10000_15000\"\n",
    "    if (x>15000 and x<=20000):\n",
    "        return \"15000_20000\"\n",
    "    if (x>20000 and x<=25000):\n",
    "        return \"20000_25000\"\n",
    "    if (x>25000 and x<=30000):\n",
    "        return \"25000_30000\"\n",
    "    if (x>35000 and x<=40000):\n",
    "        return \"35000_40000\"    \n",
    "    if (x>40000 and x<=45000):\n",
    "        return \"40000_45000\"  \n",
    "    if (x>45000 and x<=50000):\n",
    "        return \"45000_50000\"  \n",
    "    if (x>50000 ):\n",
    "        return \"x>50000\"\n",
    "        \n",
    "def BuildArea(x):\n",
    "    if x == 0:\n",
    "        return \"nan\"\n",
    "    if (x!=0 and x<=10000):\n",
    "        return \"Small\"\n",
    "    if (x>10000 and x<=50000):\n",
    "        return \"Medium\"\n",
    "    if (x>50000 ):\n",
    "        return \"Large\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS DUMMIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-23 09:17:59.356259\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef PreviousClass1TransactionsTaxLiens(df, dfc, dt, dx):\\n    dfc = dfc.rename(columns={\\'BIN-Occurrence\\': \\'BIN\\', \\'Vio Issued Date\\': \\'VioDate\\'})\\n    dfc = dfc[[\"BIN\", \"VioDate\"]]\\n    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\\n    dfc[\"BIN\"] = dfc[\"BIN\"].str.split(\".\").str[0]\\n    dfc[\\'VioDate\\'] = pd.to_datetime(dfc[\\'VioDate\\'])\\n    \\n    dt = dt[[\"RECORDED / FILED\", \"BBL\"]]\\n    dt = dt.rename(columns={\\'RECORDED / FILED\\': \\'FiledDate\\'})\\n    dt[\\'FiledDate\\'] = pd.to_datetime(dt[\\'FiledDate\\'])\\n    dt[\"BBL\"] = dt[\"BBL\"].astype(str)\\n    dt[\"BBL\"] = dt[\"BBL\"].str.split(\".\").str[0]\\n    \\n    print (\"Create bbl for dx\")\\n    dx[\"Block \"] = dx[\"Block \"].astype(str).map(str.strip)\\n    dx[\"Lot\"] = dx[\"Lot\"].astype(str).map(str.strip)\\n    dx[\"Borough\"] = dx[\"Borough\"].astype(str).map(str.strip)\\n    dx[\\'Block \\'] = dx[\\'Block \\'].str.split(\".\").str[0]\\n    dx[\\'Lot\\'] = dx[\\'Lot\\'].str.split(\".\").str[0]\\n    dx[\\'Borough\\'] = dx[\\'Borough\\'].str.split(\".\").str[0]\\n    dx[\\'Block \\'] = dx[\\'Block \\'].apply(convert_block)\\n    dx[\\'Lot\\'] = dx[\\'Lot\\'].apply(convert_lot)\\n    dx[\"BBL\"] = dx[\"Borough\"] + dx[\"Block \"] + dx[\"Lot\"]\\n    dx[\\'Month\\'] = dx[\\'Month\\'].str.replace(\\'/17/\\', \\'/1/\\')\\n    dx[\\'Month\\'] = pd.to_datetime(dx[\\'Month\\'])\\n    dx = dx[[\"Month\", \"BBL\"]]\\n\\n    df[\"BBL_SPATIAL_KEY\"] = df[\"BBL_SPATIAL_KEY\"].astype(str)\\n    df[\"BBL_SPATIAL_KEY\"] = df[\"BBL_SPATIAL_KEY\"].str.split(\".\").str[0]\\n    \\n    dfbblist = list(set(df[\"BBL_SPATIAL_KEY\"].tolist() ))\\n    print (\"len of dt before drop\", len(dt))\\n    dt = dt[dt[\\'BBL\\'].isin(dfbblist)].reset_index(drop=True)\\n    print (\"len of dt after drop\", len(dt))\\n    \\n    print (\"len of dx before drop by bbl\", len(dx))\\n    dx = dx[dx[\\'BBL\\'].isin(dfbblist)].reset_index(drop=True)\\n    print (\"len of dx after drop\", len(dx))\\n\\n    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\\n    print (\"len before drop\", len(dfc))\\n    dfc = dfc[dfc[\\'BIN\\'].isin(dfbins)].reset_index(drop=True)\\n    print (\"len after drop\", len(dfc))\\n\\n    df[\"PreviousClass1\"] = 0\\n    df[\"PreviousSale\"] = 0\\n    df[\"PreviousTaxLien\"] = 0\\n    df[\\'DateReceived\\'] = pd.to_datetime(df[\\'DateReceived\\'])\\n\\n    thebins = df[\"BIN\"].tolist()\\n    thebbls = df[\"BBL_SPATIAL_KEY\"].tolist()\\n    print (\"len of thebins\", len(thebins))\\n    print (\"len of df\", len(df))\\n    datesRec = df[\"DateReceived\"].tolist()\\n    \\n    class1VioDic = {}\\n    \\n    datethresh = 30\\n    \\n    #for i in range(0, len(thebins)):\\n    for i in range(40, 50):\\n        #b = thebins[i]\\n        #print (b)\\n        dateReceived = df[\"DateReceived\"][i]\\n        #print (dateReceived)\\n        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\\n        dt2 = dt[dt[\"BBL\"] == thebbls[i]]\\n        dx2 = dx[dx[\"BBL\"] == thebbls[i]]\\n\\n        #print (\"dfc2\")\\n        #print (dfc2)\\n        #print (\"dt2\")\\n        \\n        if len(dx2)>0:\\n            print (i)\\n            print (thebbls[i])\\n            print (dx2)\\n            print ()\\n        \\n        if i%2000==0:\\n            print (i) \\n        #if len(dfc2) > 0:\\n        mask1 = (dfc2[\\'VioDate\\'] < datesRec[i])\\n        dfc2 = dfc2.loc[mask1]\\n        \\n        mask2 = (dt2[\\'FiledDate\\'] < datesRec[i])\\n        dt2 = dt2.loc[mask2]\\n        \\n        mask3 = (dx2[\\'Month\\'] < datesRec[i])\\n        dx2 = dx2.loc[mask3]\\n        \\n        print (\"after mask filter\")\\n        print (\"date received\", datesRec[i])\\n        print (dx2)\\n        \\n        if len(dt2)>0:\\n            df[\"PreviousSale\"][i] = 1\\n        \\n\\n        if len(dfc2) > 0:\\n            #df[\"PreviousClass1\"][i] = 1\\n            #class1VioDic[thebins[i]] = 1\\n            #print ()\\n            #print (dateReceived)\\n            #print (dfc2)\\n                \\n            dfc2[\"dfDateReceived\"] = datesRec[i]\\n            dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"VioDate\"] ).dt.days\\n            dfc2 = dfc2[dfc2[\"DayDiff\"] > datethresh]\\n            #print (dfc2)\\n            if len(dfc2) > 0:\\n                df[\"PreviousClass1\"][i] = 1 \\n                    \\n    \\n    return df\\n\\n\\ndef convert_block(x):\\n    #Creates median income categorical variable\\n    if len(x) == 1:\\n        return \"0000\" + x\\n    if len(x) == 2:\\n        return \"000\" + x\\n    if len(x) == 3:\\n        return \"00\" + x\\n    if len(x) == 4:\\n        return \"0\" + x\\n    else:\\n        return x\\n    \\ndef convert_lot(x):\\n    #Creates median income categorical variable\\n    if len(x) == 1:\\n        return \"000\" + x\\n    if len(x) == 2:\\n        return \"00\" + x\\n    if len(x) == 3:\\n        return \"0\" + x\\n    else:\\n        return x\\n  \\n\\n\\nprint (\"Previous class 1 and deed trasactions dummies\")\\ndfc  = pd.read_csv(path + \"Class1_issued.csv\",encoding=\\'cp1252\\', low_memory=False)\\ndt = pd.read_csv(path + \"TRANSACTIONS 11 YEARS edited.csv\",encoding=\\'cp1252\\', low_memory=False)\\n\\ndx = pd.read_csv(path + \"Tax_Lien_Sale_Lists.csv\",encoding=\\'cp1252\\', low_memory=False)\\n\\ndf = PreviousClass1TransactionsTaxLiens(df, dfc, dt, dx)\\n\\n\\n\\ndef PreviousComplaint(df, dfc):\\n    dfc = dfc.rename(columns={\\'BIN Number\\': \\'BIN\\', \\'D_DATE\\': \\'DateReceived\\'})\\n    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\\n    \\n    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\\n    \\n    print (\"len of complaints before drop\", len(dfc) )\\n    print (\"drop based on bins in df\")\\n    dfc = dfc[dfc[\\'BIN\\'].isin(dfbins)].reset_index(drop=True)\\n    print (\"keep only illegal conversion complaints(45)\")\\n    dfc[\"Complaint  Category\"] = dfc[\"Complaint  Category\"].astype(str).map(str.strip)\\n    dfc = dfc[dfc[\"Complaint  Category\"] == \\'45\\']\\n    print (\"len of complaints after drop\", len(dfc) )\\n    dfc = dfc[[\"BIN\", \"DateReceived\"]]\\n    \\n    df[\"Previous45Complaint\"] = 0\\n    \\n    df[\\'DateReceived\\'] = pd.to_datetime(df[\\'DateReceived\\'])\\n\\n    thebins = df[\"BIN\"].tolist()\\n    datesRec = df[\"DateReceived\"].tolist()\\n    \\n    dfc[\\'DateReceived\\'] = pd.to_datetime(dfc[\\'DateReceived\\'])\\n    \\n    datethresh = 1095\\n\\n    for i in range(0, len(df)):\\n        b = df[\"BIN\"][i]\\n        #print (b)\\n        dateReceived = df[\"DateReceived\"][i]\\n        #print (dateReceived)\\n        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\\n        #print (dfc2)\\n        if i%2000==0:\\n            print (i) \\n        if len(dfc2) > 0:\\n            mask = (dfc2[\\'DateReceived\\'] < datesRec[i])\\n            dfc2 = dfc2.loc[mask]\\n            #print ()\\n            #print (dfc2)\\n            if len(dfc2) > 0:\\n                dfc2[\"dfDateReceived\"] = datesRec[i]\\n                dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"DateReceived\"] ).dt.days\\n                #print (dv2)\\n                dfc2 = dfc2[dfc2[\"DayDiff\"] < datethresh]\\n                #print (dfc2)\\n                if len(dfc2) > 0:\\n                    df[\"Previous45Complaint\"][i] = 1 \\n    return df\\n\\nprint ()\\nprint (\"Create previous complaint dummy\")\\ndfc1 = pd.read_csv(path + \"Complaints Building Census 2007-2010.csv\",encoding=\\'cp1252\\', low_memory=False)\\ndfc2 = pd.read_csv(path + \"Complaints Building Census 2011-2015.csv\",encoding=\\'cp1252\\', low_memory=False)\\ndfc3 = pd.read_csv(path + \"Complaints Building Census 2016-Present.csv\",encoding=\\'cp1252\\',low_memory=False )\\nframes = [dfc1, dfc2, dfc3]\\n\\ndfc = pd.concat(frames)\\ndfc = dfc.reset_index(drop=True)\\n\\ndfc = PreviousComplaint(df, dfc)\\n\\n\\n\\n\\ndef PreviousClass1(df, dfc):\\n    dfc = dfc.rename(columns={\\'BIN-Occurrence\\': \\'BIN\\', \\'Vio Issued Date\\': \\'VioDate\\'})\\n    dfc = dfc[[\"BIN\", \"VioDate\"]]\\n    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\\n    dfc[\"BIN\"] = dfc[\"BIN\"].str.split(\".\").str[0]\\n    dfc[\\'VioDate\\'] = pd.to_datetime(dfc[\\'VioDate\\'])\\n\\n    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\\n    print (\"len before drop\", len(dfc))\\n    dfc = dfc[dfc[\\'BIN\\'].isin(dfbins)].reset_index(drop=True)\\n    print (\"len after drop\", len(dfc))\\n    \\n    dfcList = dfc.values.tolist()\\n\\n\\n    df[\"PreviousClass1\"] = 0\\n    df[\\'DateReceived\\'] = pd.to_datetime(df[\\'DateReceived\\'])\\n\\n    thebins = df[\"BIN\"].tolist()\\n    print (\"len of thebins\", len(thebins))\\n    print (\"len of df\", len(df))\\n    datesRec = df[\"DateReceived\"].tolist()\\n    \\n    class1VioDic = {}\\n    \\n    #datethresh = 1095\\n \\n    for i in range(0, len(thebins)):\\n        #b = thebins[i]\\n        #print (b)\\n        dateReceived = df[\"DateReceived\"][i]\\n        #print (dateReceived)\\n        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\\n        #print (dfc2)\\n        if i%2000==0:\\n            print (i) \\n        #if len(dfc2) > 0:\\n        mask = (dfc2[\\'VioDate\\'] < datesRec[i])\\n        dfc2 = dfc2.loc[mask]\\n        if len(dfc2) > 0:\\n            df[\"PreviousClass1\"][i] = 1\\n            #class1VioDic[thebins[i]] = 1\\n                #print ()\\n                #print (dateReceived)\\n                #print (dfc2)\\n                \\n                #dfc2[\"dfDateReceived\"] = datesRec[i]\\n                #dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"DateReceived\"] ).dt.days\\n                #print (dv2)\\n                #dfc2 = dfc2[dfc2[\"DayDiff\"] < datethresh]\\n                #print (dfc2)\\n                #if len(dfc2) > 0:\\n                #    df[\"Previous45Complaint\"][i] = 1 \\n                    \\n\\n    return lol\\n\\ntest = PreviousClass1(df, dfc)\\n\\n\\n\\ndef PreviousVacate(df):\\n    \\n    from operator import itemgetter\\n\\n    \\n    df[\\'DateReceived\\'] = pd.to_datetime(df[\\'DateReceived\\'])\\n    #datesRec = df[\"DateReceived\"].tolist()\\n\\n    #print (\"len before drop\", len(df) )\\n    #df2 = df[df[\"Outcome\"] == \"Vacate Issued\"].reset_index(drop=True)\\n    #print (\"len after drop\", len(df2) )\\n    df2 = df[[\"BIN\", \"DateReceived\"]]\\n    df2[\"BIN\"] = df2[\"BIN\"].astype(int)\\n    thebins = df2[\"BIN\"].tolist()\\n\\n    df2 = df2.values.tolist()\\n\\n    \\n    \\n    PreviousVacateDic = {}\\n\\n    #datethresh = 1095\\n    \\n    newList = []\\n    \\n    for i in range(0, len(df2)):\\n        #b = df[\"BIN\"][i]\\n        #print (b)\\n        \\n        b = df2[i][0]\\n        \\n        for j in range(0, len(df2)):\\n            if df2[j][0] == b:\\n                #print (df2[j][0], df2[j][1])\\n                newList.append([df2[j][0], df2[j][1]])\\n        \\n        \\n        newList = sorted(newList, key=itemgetter(1), reverse=True)\\n        recentdateReceived = newList[0][1]\\n        \\n        newList2 = []\\n        for j in range(0, len(newList)):\\n            if newList[j][1] < recentdateReceived:\\n                newList2.append([newList[j][0], newList[j][1]])\\n\\n\\n        #print (b, \" \", recentdateReceived)\\n        #print (newList)\\n        #print (newList2)\\n        \\n        if i%2000==0:\\n            print (i) \\n       \\n        if len(newList2) > 0:\\n            PreviousVacateDic[b] = 1\\n        \\n        newList = []\\n        newList2 = []\\n        \\n    print (\"len of previous vacate dic\", len(PreviousVacateDic))   \\n    df[\"PreviousVacate\"] = df[\"BIN\"].map(PreviousVacateDic)\\n    df[\"PreviousVacate\"] = df[\"PreviousVacate\"].fillna(0)\\n  \\n   \\n    return df\\n\\ndt = PreviousVacate(df)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (str(datetime.now()) )\n",
    "print ()\n",
    "\n",
    "\n",
    "def NoPreviousJobFiling(df, dfj):\n",
    "    dfj[\"BIN Number\"] = dfj[\"BIN Number\"].astype(str).map(str.strip)\n",
    "    print (\"len dfj before drop\", len(dfj))\n",
    "    #binList = list(set(dfj[\"BIN Number\"].tolist()) )\n",
    "    dfj = dfj.drop_duplicates(['BIN Number']).reset_index(drop=True)\n",
    "    print (\"len dfj after drop\", len(dfj))\n",
    "    \n",
    "    dfj[\"PreviousJob\"] = 0\n",
    "    dfjDic = dfj.set_index('BIN Number')['PreviousJob'].to_dict()\n",
    "\n",
    "    df[\"NoPreviousJob\"] = df[\"BIN\"].map(dfjDic)\n",
    "    df[\"NoPreviousJob\"] = df[\"NoPreviousJob\"].fillna(1)\n",
    "    \n",
    "    print (\"len of df\", len(df))\n",
    "    \n",
    "    print (\"number of complaints on BINs with no previous job filings\", df[\"NoPreviousJob\"].sum())\n",
    "    print (\"percent of BINs with no history of job filings\", df[\"NoPreviousJob\"].sum()/len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def PreviousClass1Transactions(df, dfc, dt):\n",
    "    dfc = dfc.rename(columns={'BIN-Occurrence': 'BIN', 'Vio Issued Date': 'VioDate'})\n",
    "    dfc = dfc[[\"BIN\", \"VioDate\"]]\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].str.split(\".\").str[0]\n",
    "    dfc['VioDate'] = pd.to_datetime(dfc['VioDate'])\n",
    "    \n",
    "    dt = dt[[\"RECORDED / FILED\", \"BBL\"]]\n",
    "    dt = dt.rename(columns={'RECORDED / FILED': 'FiledDate'})\n",
    "    dt['FiledDate'] = pd.to_datetime(dt['FiledDate'])\n",
    "    dt[\"BBL\"] = dt[\"BBL\"].astype(str)\n",
    "    dt[\"BBL\"] = dt[\"BBL\"].str.split(\".\").str[0]\n",
    "    \n",
    "    df[\"BBL_SPATIAL_KEY\"] = df[\"BBL_SPATIAL_KEY\"].astype(str)\n",
    "    df[\"BBL_SPATIAL_KEY\"] = df[\"BBL_SPATIAL_KEY\"].str.split(\".\").str[0]\n",
    "    \n",
    "    dfbblist = list(set(df[\"BBL_SPATIAL_KEY\"].tolist() ))\n",
    "    print (\"len of dt before drop\", len(dt))\n",
    "    dt = dt[dt['BBL'].isin(dfbblist)].reset_index(drop=True)\n",
    "    print (\"len of dt after drop\", len(dt))\n",
    "\n",
    "    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\n",
    "    print (\"len before drop\", len(dfc))\n",
    "    dfc = dfc[dfc['BIN'].isin(dfbins)].reset_index(drop=True)\n",
    "    print (\"len after drop\", len(dfc))\n",
    "\n",
    "    df[\"PreviousClass1\"] = 0\n",
    "    df[\"PreviousSale\"] = 0\n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "\n",
    "    thebins = df[\"BIN\"].tolist()\n",
    "    thebbls = df[\"BBL_SPATIAL_KEY\"].tolist()\n",
    "    print (\"len of thebins\", len(thebins))\n",
    "    print (\"len of df\", len(df))\n",
    "    datesRec = df[\"DateReceived\"].tolist()\n",
    "    \n",
    "    class1VioDic = {}\n",
    "    \n",
    "    datethresh = 30\n",
    "    \n",
    "    for i in range(0, len(thebins)):\n",
    "        #b = thebins[i]\n",
    "        #print (b)\n",
    "        dateReceived = df[\"DateReceived\"][i]\n",
    "        #print (dateReceived)\n",
    "        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\n",
    "        dt2 = dt[dt[\"BBL\"] == thebbls[i]]\n",
    "        #print (\"dfc2\")\n",
    "        #print (dfc2)\n",
    "        #print (\"dt2\")\n",
    "        #print (thebbls[i])\n",
    "        #print (dt2)\n",
    "        \n",
    "        if i%2000==0:\n",
    "            print (i) \n",
    "        #if len(dfc2) > 0:\n",
    "        mask1 = (dfc2['VioDate'] < datesRec[i])\n",
    "        dfc2 = dfc2.loc[mask1]\n",
    "        \n",
    "        mask2 = (dt2['FiledDate'] < datesRec[i])\n",
    "        dt2 = dt2.loc[mask2]\n",
    "        \n",
    "        #print (\"after mask filter\")\n",
    "        #print (\"date received\", datesRec[i])\n",
    "        #print (dt2)\n",
    "        \n",
    "        if len(dt2)>0:\n",
    "            df[\"PreviousSale\"][i] = 1\n",
    "        \n",
    "\n",
    "        if len(dfc2) > 0:\n",
    "            #df[\"PreviousClass1\"][i] = 1\n",
    "            #class1VioDic[thebins[i]] = 1\n",
    "            #print ()\n",
    "            #print (dateReceived)\n",
    "            #print (dfc2)\n",
    "                \n",
    "            dfc2[\"dfDateReceived\"] = datesRec[i]\n",
    "            dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"VioDate\"] ).dt.days\n",
    "            dfc2 = dfc2[dfc2[\"DayDiff\"] > datethresh]\n",
    "            #print (dfc2)\n",
    "            if len(dfc2) > 0:\n",
    "                df[\"PreviousClass1\"][i] = 1 \n",
    "                    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def PreviousVacate(df):\n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "    #datesRec = df[\"DateReceived\"].tolist()\n",
    "\n",
    "    #print (\"len before drop\", len(df) )\n",
    "    #df2 = df[df[\"Outcome\"] == \"Vacate Issued\"].reset_index(drop=True)\n",
    "    #print (\"len after drop\", len(df2) )\n",
    "    df2 = df[[\"BIN\", \"DateReceived\"]]\n",
    "    \n",
    "    thebins = df2[\"BIN\"].tolist()\n",
    "    \n",
    "    PreviousVacateDic = {}\n",
    "\n",
    "    #datethresh = 1095\n",
    "    \n",
    "    for i in range(0, len(thebins)):\n",
    "        b = df[\"BIN\"][i]\n",
    "        #print (b)\n",
    "        df3 = df2[df2[\"BIN\"] == thebins[i]].reset_index(drop=True)\n",
    "        df3 = df3.sort_values(by = 'DateReceived', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        #print (len(df3))\n",
    "        \n",
    "    \n",
    "        dateReceived = df3[\"DateReceived\"][0]\n",
    "        #print (dateReceived)\n",
    "        if i%2000==0:\n",
    "            print (i) \n",
    "        mask = (df3['DateReceived'] < dateReceived)\n",
    "        \n",
    "        df3 = df3.loc[mask]\n",
    "        \n",
    "        if len(df3) > 0:\n",
    "            PreviousVacateDic[b] = 1\n",
    "        \n",
    "    print (\"len of previous vacate dic\", len(PreviousVacateDic))   \n",
    "    df[\"PreviousVacate\"] = df[\"BIN\"].map(PreviousVacateDic)\n",
    "    df[\"PreviousVacate\"] = df[\"PreviousVacate\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "def TextDummies(df):\n",
    "    df[\"ComplaintReason\"] = df[\"ComplaintReason\"].astype(str).map(str.strip).map(str.lower)\n",
    "    print (\"remove puncuation\")\n",
    "    df[\"ComplaintReason\"] = df[\"ComplaintReason\"].str.replace('[^\\w\\s]','')\n",
    "    print (\"remove numbers\")\n",
    "    df[\"ComplaintReason\"] = df[\"ComplaintReason\"].str.replace('\\d+', '')\n",
    "    df[\"ComplaintReason\"] = df[\"ComplaintReason\"].str.replace('house', 'home')\n",
    "    df[\"ComplaintReason\"] = df[\"ComplaintReason\"].str.replace('bsmt', 'basement')\n",
    "\n",
    "    pd.set_option('chained_assignment', None)\n",
    "\n",
    "    print (\"create text dummies\")\n",
    "    \n",
    "    #df[\"txt_home\"] = 0\n",
    "    #df[\"txt_back\"] = 0\n",
    "    #df[\"txt_garage\"] = 0\n",
    "    #df[\"txt_living\"] = 0\n",
    "    \n",
    "    df[\"txt_fdny\"] = 0\n",
    "    df[\"txt_occupancy\"] = 0\n",
    "    df[\"txt_cellar\"] = 0\n",
    "    df[\"txt_attic\"] = 0\n",
    "    df[\"txt_basement\"] = 0\n",
    "    \n",
    "\n",
    "    sumtext = [0]\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        if i%6000 == 0:\n",
    "            print (i)\n",
    "        if \"fdny\" in df[\"ComplaintReason\"][i]:\n",
    "            df[\"txt_fdny\"][i] = 1\n",
    "            sumtext.append(1)\n",
    "        #if (\"back\" in df[\"ComplaintReason\"][i] or \"rear\" in df[\"ComplaintReason\"][i]):\n",
    "        #    df[\"txt_back\"][i] = 1\n",
    "        #    sumtext.append(1)\n",
    "        if \"occupanc\" in df[\"ComplaintReason\"][i]:\n",
    "            df[\"txt_occupancy\"][i] = 1\n",
    "            sumtext.append(1)\n",
    "        if \"cellar\" in df[\"ComplaintReason\"][i]:\n",
    "            df[\"txt_cellar\"][i] = 1\n",
    "            sumtext.append(1)\n",
    "        if \"attic\" in df[\"ComplaintReason\"][i]:\n",
    "            df[\"txt_attic\"][i] = 1\n",
    "            sumtext.append(1)\n",
    "        if \"basement\" in df[\"ComplaintReason\"][i]:\n",
    "            df[\"txt_basement\"][i] = 1\n",
    "            sumtext.append(1)\n",
    " \n",
    "    return df\n",
    "\n",
    "\n",
    "def OtherDummies(df):\n",
    "    #year built, building area, median income, building class, owner type\n",
    "    print (\"Year built\")\n",
    "    dummies = pd.get_dummies(df.YearBuiltCat, prefix='YB')\n",
    "    dummies = dummies.drop('YB_nan', 1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    print (\"Building area\")\n",
    "    dummies = pd.get_dummies(df.BuildingAreaCat, prefix='BA')\n",
    "    dummies = dummies.drop('BA_nan', 1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    print (\"Median income\")\n",
    "    dummies = pd.get_dummies(df.MedianIncomeCategory, prefix='MedIncome')\n",
    "    dummies = dummies.drop('MedIncome_NAN', 1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    print (\"Building class\")\n",
    "    dummies = pd.get_dummies(df.BldgClass, prefix='BC')\n",
    "    #dummies = dummies.drop('BC_Other', 1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    print (\"Owner type\")\n",
    "    dummies = pd.get_dummies(df.OwnerType, prefix='OT')\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def CommBoardDummies(df):\n",
    "    df[\"CommunityBoard\"] = df[\"CommunityBoard\"].astype(str)\n",
    "    df[\"CommunityBoard\"] = df[\"CommunityBoard\"].str.split(\".\").str[0]\n",
    "    dummies = pd.get_dummies(df.CommunityBoard, prefix='CB')\n",
    "    dummies = dummies.drop('CB_nan', 1)\n",
    "\n",
    "    print (\"only keep cbs 10 or greater\")\n",
    "\n",
    "    cols = dummies.columns\n",
    "    dummyList = []\n",
    "    #remove columns in the dummies dataframe that are less then 10, \n",
    "    #need to reduce the number of dummies so model is not overfitted\n",
    "    for c in cols:\n",
    "        #print (dummies[c].sum() )\n",
    "\n",
    "        if dummies[c].sum() > 10:\n",
    "            #dummies = dummies.drop(c, 1)\n",
    "            dummyList.append(c)\n",
    "\n",
    "    dummies = dummies[dummyList]\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def OwnerDummies(df):\n",
    "    dummies = pd.get_dummies(df.Owner, prefix='O')\n",
    "    print (\"len of dummies\", len(dummies))\n",
    "\n",
    "    cols = dummies.columns\n",
    "    dummyList = []\n",
    "    #remove columns in the dummies dataframe that are less then 10, \n",
    "    #need to reduce the number of dummies so model is not overfitted\n",
    "    for c in cols:\n",
    "        #print (dummies[c].sum() )\n",
    "\n",
    "        if dummies[c].sum() > 10:\n",
    "            #dummies = dummies.drop(c, 1)\n",
    "            dummyList.append(c)\n",
    "\n",
    "    #remove garbage\n",
    "    #dummyList.remove(\"O_#_#\")\n",
    "    #final list of owner dummies\n",
    "    dummiesown = dummies[dummyList]\n",
    "    df = pd.concat([df, dummiesown], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def PreviousClass1TransactionsTaxLiens(df, dfc, dt, dx):\n",
    "    dfc = dfc.rename(columns={'BIN-Occurrence': 'BIN', 'Vio Issued Date': 'VioDate'})\n",
    "    dfc = dfc[[\"BIN\", \"VioDate\"]]\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].str.split(\".\").str[0]\n",
    "    dfc['VioDate'] = pd.to_datetime(dfc['VioDate'])\n",
    "    \n",
    "    dt = dt[[\"RECORDED / FILED\", \"BBL\"]]\n",
    "    dt = dt.rename(columns={'RECORDED / FILED': 'FiledDate'})\n",
    "    dt['FiledDate'] = pd.to_datetime(dt['FiledDate'])\n",
    "    dt[\"BBL\"] = dt[\"BBL\"].astype(str)\n",
    "    dt[\"BBL\"] = dt[\"BBL\"].str.split(\".\").str[0]\n",
    "    \n",
    "    print (\"Create bbl for dx\")\n",
    "    dx[\"Block \"] = dx[\"Block \"].astype(str).map(str.strip)\n",
    "    dx[\"Lot\"] = dx[\"Lot\"].astype(str).map(str.strip)\n",
    "    dx[\"Borough\"] = dx[\"Borough\"].astype(str).map(str.strip)\n",
    "    dx['Block '] = dx['Block '].str.split(\".\").str[0]\n",
    "    dx['Lot'] = dx['Lot'].str.split(\".\").str[0]\n",
    "    dx['Borough'] = dx['Borough'].str.split(\".\").str[0]\n",
    "    dx['Block '] = dx['Block '].apply(convert_block)\n",
    "    dx['Lot'] = dx['Lot'].apply(convert_lot)\n",
    "    dx[\"BBL\"] = dx[\"Borough\"] + dx[\"Block \"] + dx[\"Lot\"]\n",
    "    dx['Month'] = dx['Month'].str.replace('/17/', '/1/')\n",
    "    dx['Month'] = pd.to_datetime(dx['Month'])\n",
    "    dx = dx[[\"Month\", \"BBL\"]]\n",
    "\n",
    "    df[\"BBL_SPATIAL_KEY\"] = df[\"BBL_SPATIAL_KEY\"].astype(str)\n",
    "    df[\"BBL_SPATIAL_KEY\"] = df[\"BBL_SPATIAL_KEY\"].str.split(\".\").str[0]\n",
    "    \n",
    "    dfbblist = list(set(df[\"BBL_SPATIAL_KEY\"].tolist() ))\n",
    "    print (\"len of dt before drop\", len(dt))\n",
    "    dt = dt[dt['BBL'].isin(dfbblist)].reset_index(drop=True)\n",
    "    print (\"len of dt after drop\", len(dt))\n",
    "    \n",
    "    print (\"len of dx before drop by bbl\", len(dx))\n",
    "    dx = dx[dx['BBL'].isin(dfbblist)].reset_index(drop=True)\n",
    "    print (\"len of dx after drop\", len(dx))\n",
    "\n",
    "    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\n",
    "    print (\"len before drop\", len(dfc))\n",
    "    dfc = dfc[dfc['BIN'].isin(dfbins)].reset_index(drop=True)\n",
    "    print (\"len after drop\", len(dfc))\n",
    "\n",
    "    df[\"PreviousClass1\"] = 0\n",
    "    df[\"PreviousSale\"] = 0\n",
    "    df[\"PreviousTaxLien\"] = 0\n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "\n",
    "    thebins = df[\"BIN\"].tolist()\n",
    "    thebbls = df[\"BBL_SPATIAL_KEY\"].tolist()\n",
    "    print (\"len of thebins\", len(thebins))\n",
    "    print (\"len of df\", len(df))\n",
    "    datesRec = df[\"DateReceived\"].tolist()\n",
    "    \n",
    "    class1VioDic = {}\n",
    "    \n",
    "    datethresh = 30\n",
    "    \n",
    "    #for i in range(0, len(thebins)):\n",
    "    for i in range(40, 50):\n",
    "        #b = thebins[i]\n",
    "        #print (b)\n",
    "        dateReceived = df[\"DateReceived\"][i]\n",
    "        #print (dateReceived)\n",
    "        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\n",
    "        dt2 = dt[dt[\"BBL\"] == thebbls[i]]\n",
    "        dx2 = dx[dx[\"BBL\"] == thebbls[i]]\n",
    "\n",
    "        #print (\"dfc2\")\n",
    "        #print (dfc2)\n",
    "        #print (\"dt2\")\n",
    "        \n",
    "        if len(dx2)>0:\n",
    "            print (i)\n",
    "            print (thebbls[i])\n",
    "            print (dx2)\n",
    "            print ()\n",
    "        \n",
    "        if i%2000==0:\n",
    "            print (i) \n",
    "        #if len(dfc2) > 0:\n",
    "        mask1 = (dfc2['VioDate'] < datesRec[i])\n",
    "        dfc2 = dfc2.loc[mask1]\n",
    "        \n",
    "        mask2 = (dt2['FiledDate'] < datesRec[i])\n",
    "        dt2 = dt2.loc[mask2]\n",
    "        \n",
    "        mask3 = (dx2['Month'] < datesRec[i])\n",
    "        dx2 = dx2.loc[mask3]\n",
    "        \n",
    "        print (\"after mask filter\")\n",
    "        print (\"date received\", datesRec[i])\n",
    "        print (dx2)\n",
    "        \n",
    "        if len(dt2)>0:\n",
    "            df[\"PreviousSale\"][i] = 1\n",
    "        \n",
    "\n",
    "        if len(dfc2) > 0:\n",
    "            #df[\"PreviousClass1\"][i] = 1\n",
    "            #class1VioDic[thebins[i]] = 1\n",
    "            #print ()\n",
    "            #print (dateReceived)\n",
    "            #print (dfc2)\n",
    "                \n",
    "            dfc2[\"dfDateReceived\"] = datesRec[i]\n",
    "            dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"VioDate\"] ).dt.days\n",
    "            dfc2 = dfc2[dfc2[\"DayDiff\"] > datethresh]\n",
    "            #print (dfc2)\n",
    "            if len(dfc2) > 0:\n",
    "                df[\"PreviousClass1\"][i] = 1 \n",
    "                    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_block(x):\n",
    "    #Creates median income categorical variable\n",
    "    if len(x) == 1:\n",
    "        return \"0000\" + x\n",
    "    if len(x) == 2:\n",
    "        return \"000\" + x\n",
    "    if len(x) == 3:\n",
    "        return \"00\" + x\n",
    "    if len(x) == 4:\n",
    "        return \"0\" + x\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def convert_lot(x):\n",
    "    #Creates median income categorical variable\n",
    "    if len(x) == 1:\n",
    "        return \"000\" + x\n",
    "    if len(x) == 2:\n",
    "        return \"00\" + x\n",
    "    if len(x) == 3:\n",
    "        return \"0\" + x\n",
    "    else:\n",
    "        return x\n",
    "  \n",
    "\n",
    "\n",
    "print (\"Previous class 1 and deed trasactions dummies\")\n",
    "dfc  = pd.read_csv(path + \"Class1_issued.csv\",encoding='cp1252', low_memory=False)\n",
    "dt = pd.read_csv(path + \"TRANSACTIONS 11 YEARS edited.csv\",encoding='cp1252', low_memory=False)\n",
    "\n",
    "dx = pd.read_csv(path + \"Tax_Lien_Sale_Lists.csv\",encoding='cp1252', low_memory=False)\n",
    "\n",
    "df = PreviousClass1TransactionsTaxLiens(df, dfc, dt, dx)\n",
    "\n",
    "\n",
    "\n",
    "def PreviousComplaint(df, dfc):\n",
    "    dfc = dfc.rename(columns={'BIN Number': 'BIN', 'D_DATE': 'DateReceived'})\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\n",
    "    \n",
    "    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\n",
    "    \n",
    "    print (\"len of complaints before drop\", len(dfc) )\n",
    "    print (\"drop based on bins in df\")\n",
    "    dfc = dfc[dfc['BIN'].isin(dfbins)].reset_index(drop=True)\n",
    "    print (\"keep only illegal conversion complaints(45)\")\n",
    "    dfc[\"Complaint  Category\"] = dfc[\"Complaint  Category\"].astype(str).map(str.strip)\n",
    "    dfc = dfc[dfc[\"Complaint  Category\"] == '45']\n",
    "    print (\"len of complaints after drop\", len(dfc) )\n",
    "    dfc = dfc[[\"BIN\", \"DateReceived\"]]\n",
    "    \n",
    "    df[\"Previous45Complaint\"] = 0\n",
    "    \n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "\n",
    "    thebins = df[\"BIN\"].tolist()\n",
    "    datesRec = df[\"DateReceived\"].tolist()\n",
    "    \n",
    "    dfc['DateReceived'] = pd.to_datetime(dfc['DateReceived'])\n",
    "    \n",
    "    datethresh = 1095\n",
    "\n",
    "    for i in range(0, len(df)):\n",
    "        b = df[\"BIN\"][i]\n",
    "        #print (b)\n",
    "        dateReceived = df[\"DateReceived\"][i]\n",
    "        #print (dateReceived)\n",
    "        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\n",
    "        #print (dfc2)\n",
    "        if i%2000==0:\n",
    "            print (i) \n",
    "        if len(dfc2) > 0:\n",
    "            mask = (dfc2['DateReceived'] < datesRec[i])\n",
    "            dfc2 = dfc2.loc[mask]\n",
    "            #print ()\n",
    "            #print (dfc2)\n",
    "            if len(dfc2) > 0:\n",
    "                dfc2[\"dfDateReceived\"] = datesRec[i]\n",
    "                dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"DateReceived\"] ).dt.days\n",
    "                #print (dv2)\n",
    "                dfc2 = dfc2[dfc2[\"DayDiff\"] < datethresh]\n",
    "                #print (dfc2)\n",
    "                if len(dfc2) > 0:\n",
    "                    df[\"Previous45Complaint\"][i] = 1 \n",
    "    return df\n",
    "\n",
    "print ()\n",
    "print (\"Create previous complaint dummy\")\n",
    "dfc1 = pd.read_csv(path + \"Complaints Building Census 2007-2010.csv\",encoding='cp1252', low_memory=False)\n",
    "dfc2 = pd.read_csv(path + \"Complaints Building Census 2011-2015.csv\",encoding='cp1252', low_memory=False)\n",
    "dfc3 = pd.read_csv(path + \"Complaints Building Census 2016-Present.csv\",encoding='cp1252',low_memory=False )\n",
    "frames = [dfc1, dfc2, dfc3]\n",
    "\n",
    "dfc = pd.concat(frames)\n",
    "dfc = dfc.reset_index(drop=True)\n",
    "\n",
    "dfc = PreviousComplaint(df, dfc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PreviousClass1(df, dfc):\n",
    "    dfc = dfc.rename(columns={'BIN-Occurrence': 'BIN', 'Vio Issued Date': 'VioDate'})\n",
    "    dfc = dfc[[\"BIN\", \"VioDate\"]]\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].astype(str)\n",
    "    dfc[\"BIN\"] = dfc[\"BIN\"].str.split(\".\").str[0]\n",
    "    dfc['VioDate'] = pd.to_datetime(dfc['VioDate'])\n",
    "\n",
    "    dfbins = sorted(list(set(df[\"BIN\"].tolist())))\n",
    "    print (\"len before drop\", len(dfc))\n",
    "    dfc = dfc[dfc['BIN'].isin(dfbins)].reset_index(drop=True)\n",
    "    print (\"len after drop\", len(dfc))\n",
    "    \n",
    "    dfcList = dfc.values.tolist()\n",
    "\n",
    "\n",
    "    df[\"PreviousClass1\"] = 0\n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "\n",
    "    thebins = df[\"BIN\"].tolist()\n",
    "    print (\"len of thebins\", len(thebins))\n",
    "    print (\"len of df\", len(df))\n",
    "    datesRec = df[\"DateReceived\"].tolist()\n",
    "    \n",
    "    class1VioDic = {}\n",
    "    \n",
    "    #datethresh = 1095\n",
    " \n",
    "    for i in range(0, len(thebins)):\n",
    "        #b = thebins[i]\n",
    "        #print (b)\n",
    "        dateReceived = df[\"DateReceived\"][i]\n",
    "        #print (dateReceived)\n",
    "        dfc2 = dfc[dfc[\"BIN\"] == thebins[i]]\n",
    "        #print (dfc2)\n",
    "        if i%2000==0:\n",
    "            print (i) \n",
    "        #if len(dfc2) > 0:\n",
    "        mask = (dfc2['VioDate'] < datesRec[i])\n",
    "        dfc2 = dfc2.loc[mask]\n",
    "        if len(dfc2) > 0:\n",
    "            df[\"PreviousClass1\"][i] = 1\n",
    "            #class1VioDic[thebins[i]] = 1\n",
    "                #print ()\n",
    "                #print (dateReceived)\n",
    "                #print (dfc2)\n",
    "                \n",
    "                #dfc2[\"dfDateReceived\"] = datesRec[i]\n",
    "                #dfc2[\"DayDiff\"] = (dfc2[\"dfDateReceived\"] - dfc2[\"DateReceived\"] ).dt.days\n",
    "                #print (dv2)\n",
    "                #dfc2 = dfc2[dfc2[\"DayDiff\"] < datethresh]\n",
    "                #print (dfc2)\n",
    "                #if len(dfc2) > 0:\n",
    "                #    df[\"Previous45Complaint\"][i] = 1 \n",
    "                    \n",
    "\n",
    "    return lol\n",
    "\n",
    "test = PreviousClass1(df, dfc)\n",
    "\n",
    "\n",
    "\n",
    "def PreviousVacate(df):\n",
    "    \n",
    "    from operator import itemgetter\n",
    "\n",
    "    \n",
    "    df['DateReceived'] = pd.to_datetime(df['DateReceived'])\n",
    "    #datesRec = df[\"DateReceived\"].tolist()\n",
    "\n",
    "    #print (\"len before drop\", len(df) )\n",
    "    #df2 = df[df[\"Outcome\"] == \"Vacate Issued\"].reset_index(drop=True)\n",
    "    #print (\"len after drop\", len(df2) )\n",
    "    df2 = df[[\"BIN\", \"DateReceived\"]]\n",
    "    df2[\"BIN\"] = df2[\"BIN\"].astype(int)\n",
    "    thebins = df2[\"BIN\"].tolist()\n",
    "\n",
    "    df2 = df2.values.tolist()\n",
    "\n",
    "    \n",
    "    \n",
    "    PreviousVacateDic = {}\n",
    "\n",
    "    #datethresh = 1095\n",
    "    \n",
    "    newList = []\n",
    "    \n",
    "    for i in range(0, len(df2)):\n",
    "        #b = df[\"BIN\"][i]\n",
    "        #print (b)\n",
    "        \n",
    "        b = df2[i][0]\n",
    "        \n",
    "        for j in range(0, len(df2)):\n",
    "            if df2[j][0] == b:\n",
    "                #print (df2[j][0], df2[j][1])\n",
    "                newList.append([df2[j][0], df2[j][1]])\n",
    "        \n",
    "        \n",
    "        newList = sorted(newList, key=itemgetter(1), reverse=True)\n",
    "        recentdateReceived = newList[0][1]\n",
    "        \n",
    "        newList2 = []\n",
    "        for j in range(0, len(newList)):\n",
    "            if newList[j][1] < recentdateReceived:\n",
    "                newList2.append([newList[j][0], newList[j][1]])\n",
    "\n",
    "\n",
    "        #print (b, \" \", recentdateReceived)\n",
    "        #print (newList)\n",
    "        #print (newList2)\n",
    "        \n",
    "        if i%2000==0:\n",
    "            print (i) \n",
    "       \n",
    "        if len(newList2) > 0:\n",
    "            PreviousVacateDic[b] = 1\n",
    "        \n",
    "        newList = []\n",
    "        newList2 = []\n",
    "        \n",
    "    print (\"len of previous vacate dic\", len(PreviousVacateDic))   \n",
    "    df[\"PreviousVacate\"] = df[\"BIN\"].map(PreviousVacateDic)\n",
    "    df[\"PreviousVacate\"] = df[\"PreviousVacate\"].fillna(0)\n",
    "  \n",
    "   \n",
    "    return df\n",
    "\n",
    "dt = PreviousVacate(df)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-23 09:18:03.013567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (str(datetime.now()) )\n",
    "print ()\n",
    "\n",
    "def RandomUpsampleTraining(df2):\n",
    "    print (\"create open complaint dataset and then train, validation and test datasets\")\n",
    "\n",
    "    opendf = df2[df2[\"Outcome\"] == \"Open\"]\n",
    "    print (\"len of opendf\", len(opendf))\n",
    "\n",
    "    df2 = df2[df2[\"Outcome\"] != \"Open\"].reset_index(drop=True)\n",
    "    print (\"len of df2\", len(df2))\n",
    "    \n",
    "    #convert swo issued/not issued to 0/1\n",
    "    df2 = OutcomeConvert(df2)\n",
    "    \n",
    "    df2[\"Index\"] = df2.index\n",
    "\n",
    "\n",
    "    def BalanceClass(df2):\n",
    "        # Separate majority and minority classes\n",
    "        df_majority = df2[df2.Outcome==0]\n",
    "        df_minority = df2[df2.Outcome==1]\n",
    "\n",
    "        print (\"Size of majority class\", len(df_majority) )\n",
    "        print (\"Size of minority class\", len(df_minority) )\n",
    "\n",
    "        lenMajority = len(df_majority)\n",
    "\n",
    "\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority, \n",
    "                                         replace=True,     # sample with replacement\n",
    "                                         n_samples=lenMajority, # to match majority class\n",
    "                                         random_state=123) # reproducible results\n",
    "\n",
    "        print (\"Size of minority class upsampled\", len(df_minority_upsampled) )\n",
    "\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        df2 = pd.concat([df_majority, df_minority_upsampled])\n",
    "        df2 = df2.reset_index(drop=True)\n",
    "\n",
    "        df2 = df2.fillna(0)\n",
    "        print (\"Size of final dataset \", len(df2) )\n",
    "\n",
    "\n",
    "        return df2\n",
    "\n",
    "\n",
    "    print (\"Get holdout data that is not upsampled\")\n",
    "\n",
    "    print (\"Randomly sort df2\" )\n",
    "    #print (\"NOT RANDOMLY SORTING BEFORE UPSAMPLE\")\n",
    "    #df2bal[\"Index\"] = df2bal.index\n",
    "    df2 = df2.sample(frac=1)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    lengthdf2 = int(len(df2))\n",
    "    lengthdf2_20perc = int(lengthdf2*0.20)\n",
    "    lengthdf2_60perc = int(lengthdf2*0.60)\n",
    "\n",
    "    #get non-upsampled \n",
    "    print (\"get the imbalanced holdout and test\")\n",
    "    df2hold = df2[lengthdf2_60perc + lengthdf2_20perc:]\n",
    "    print (\"df2hold\", len(df2hold))\n",
    "    \n",
    "    df2testImbal = df2[lengthdf2_60perc:lengthdf2_60perc + lengthdf2_20perc]\n",
    "    print (\"df2testImbal\", len(df2testImbal))\n",
    "\n",
    "    print (\"Get the training then upsample training only\")\n",
    "    #df3 = df2[:lengthdf2_60perc + lengthdf2_20perc]\n",
    "    df3 = df2[:lengthdf2_60perc]\n",
    "\n",
    "    print (\"Balance the class\")\n",
    "    df2bal = BalanceClass(df3)\n",
    "\n",
    "    print (\"Randomly sort df2bal\" )\n",
    "    #df2bal[\"Index\"] = df2bal.index\n",
    "    df2bal = df2bal.sample(frac=1)\n",
    "    df2bal = df2bal.reset_index(drop=True)\n",
    "\n",
    "    #lengthdf2 = int(len(df2bal))\n",
    "    #lengthdf2_20perc = int(lengthdf2*0.20)\n",
    "    #lengthdf2_60perc = int(lengthdf2*0.60)\n",
    "\n",
    "    #print (\"lengthdf2bal\", lengthdf2)\n",
    "    #print (\"lengthdf2bal_60perc\",  lengthdf2_60perc)\n",
    "    #print ( \"lengthdf2bal_20perc\", lengthdf2_20perc )\n",
    "\n",
    "\n",
    "    #df2trainBal = df2bal[0:lengthdf2_60perc]\n",
    "    df2trainBal = df2bal.copy()\n",
    "    print (\"df2trainBal\", len(df2trainBal))\n",
    "    \n",
    "\n",
    "    #df2holdBal = df2bal[lengthdf2_60perc + lengthdf2_20perc:]\n",
    "    #print (\"df2holdBal\", len(df2holdBal))\n",
    "    \n",
    "    return df2trainBal, df2testImbal, df2hold, opendf\n",
    "\n",
    "\n",
    "\n",
    "def RandomUpsampleTrainingTest(df2):\n",
    "    print (\"create open complaint dataset and then train, validation and test datasets\")\n",
    "\n",
    "    opendf = df2[df2[\"Outcome\"] == \"Open\"]\n",
    "    print (\"len of opendf\", len(opendf))\n",
    "\n",
    "    df2 = df2[df2[\"Outcome\"] != \"Open\"].reset_index(drop=True)\n",
    "    print (\"len of df2\", len(df2))\n",
    "    \n",
    "    #convert swo issued/not issued to 0/1\n",
    "    df2 = OutcomeConvert(df2)\n",
    "    \n",
    "    df2[\"Index\"] = df2.index\n",
    "\n",
    "\n",
    "    def BalanceClass(df2):\n",
    "        # Separate majority and minority classes\n",
    "        df_majority = df2[df2.Outcome==0]\n",
    "        df_minority = df2[df2.Outcome==1]\n",
    "\n",
    "        print (\"Size of majority class\", len(df_majority) )\n",
    "        print (\"Size of minority class\", len(df_minority) )\n",
    "\n",
    "        lenMajority = len(df_majority)\n",
    "\n",
    "\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority, \n",
    "                                         replace=True,     # sample with replacement\n",
    "                                         n_samples=lenMajority, # to match majority class\n",
    "                                         random_state=123) # reproducible results\n",
    "\n",
    "        print (\"Size of minority class upsampled\", len(df_minority_upsampled) )\n",
    "\n",
    "\n",
    "        # Combine majority class with upsampled minority class\n",
    "        df2 = pd.concat([df_majority, df_minority_upsampled])\n",
    "        df2 = df2.reset_index(drop=True)\n",
    "\n",
    "        df2 = df2.fillna(0)\n",
    "        print (\"Size of final dataset \", len(df2) )\n",
    "\n",
    "\n",
    "        return df2\n",
    "\n",
    "\n",
    "    print (\"Get holdout data that is not upsampled\")\n",
    "\n",
    "    print (\"Randomly sort df2\" )\n",
    "    #df2bal[\"Index\"] = df2bal.index\n",
    "    df2 = df2.sample(frac=1)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    lengthdf2 = int(len(df2))\n",
    "    lengthdf2_20perc = int(lengthdf2*0.20)\n",
    "    lengthdf2_60perc = int(lengthdf2*0.60)\n",
    "\n",
    "    #get non-upsampled \n",
    "    df2hold = df2[lengthdf2_60perc + lengthdf2_20perc:]\n",
    "    print (\"df2hold\", len(df2hold))\n",
    "\n",
    "    print (\"Get the training and test dataset and then upsample\")\n",
    "    df3 = df2[:lengthdf2_60perc + lengthdf2_20perc]\n",
    "\n",
    "\n",
    "    print (\"Balance the class\")\n",
    "    df2bal = BalanceClass(df3)\n",
    "\n",
    "    print (\"Randomly sort df2bal\" )\n",
    "    #df2bal[\"Index\"] = df2bal.index\n",
    "    df2bal = df2bal.sample(frac=1)\n",
    "    df2bal = df2bal.reset_index(drop=True)\n",
    "\n",
    "    lengthdf2 = int(len(df2bal))\n",
    "    lengthdf2_20perc = int(lengthdf2*0.20)\n",
    "    lengthdf2_60perc = int(lengthdf2*0.60)\n",
    "\n",
    "    print (\"lengthdf2bal\", lengthdf2)\n",
    "    print (\"lengthdf2bal_60perc\",  lengthdf2_60perc)\n",
    "    print ( \"lengthdf2bal_20perc\", lengthdf2_20perc )\n",
    "\n",
    "\n",
    "    df2trainBal = df2bal[0:lengthdf2_60perc]\n",
    "    print (\"df2trainBal\", len(df2trainBal))\n",
    "    df2testBal = df2bal[lengthdf2_60perc:lengthdf2_60perc + lengthdf2_20perc]\n",
    "    print (\"df2testBal\", len(df2testBal))\n",
    "    #df2holdBal = df2bal[lengthdf2_60perc + lengthdf2_20perc:]\n",
    "    #print (\"df2holdBal\", len(df2holdBal))\n",
    "    \n",
    "    return df2trainBal, df2testBal, df2hold\n",
    "\n",
    "\n",
    "def DropFields(f):\n",
    "    f = f.drop('BuildingArea', 1)\n",
    "    f = f.drop('YearBuilt', 1)\n",
    "    f = f.drop('P_NUMBER_OF_STORIES', 1)\n",
    "    f = f.drop('MedianIncome', 1)\n",
    "    f = f.drop('YearBuiltCat', 1)\n",
    "    f = f.drop('BuildingAreaCat', 1)\n",
    "    f = f.drop('MedianIncomeCategory', 1)\n",
    "    f = f.drop('OwnerType', 1)\n",
    "    f = f.drop('BldgClass', 1)\n",
    "    f = f.drop('CommunityBoard', 1)\n",
    "    f = f.drop('Owner', 1)\n",
    "    f = f.drop('DateReceived', 1)\n",
    "    f = f.drop('ComplaintCategory', 1)\n",
    "    f = f.drop('ComplaintCategoryDescription', 1)\n",
    "    f = f.drop('ComplaintNumber', 1)\n",
    "    f = f.drop('BIN', 1)\n",
    "    f = f.drop('BoroughName', 1)\n",
    "    f = f.drop('Latitude', 1)\n",
    "    f = f.drop('Longitude', 1)\n",
    "    f = f.drop('DOFBuildingClassification', 1)\n",
    "    f = f.drop('FinanceOwnerName', 1)\n",
    "    f = f.drop('HouseNumber', 1)\n",
    "    f = f.drop('StreetName', 1)\n",
    "    f = f.drop('RegistrationID', 1)\n",
    "    f = f.drop('Index', 1)\n",
    "    f = f.drop('SourceofComplaintDescription', 1)\n",
    "    f = f.drop('ComplaintReason', 1)\n",
    "    f = f.drop('BBL_SPATIAL_KEY', 1)\n",
    "\n",
    "     \n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION CALLS DATA PREPERATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DOBUSER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read violations data\n",
      "Clean vio data\n",
      "len before date drop 54377\n",
      "len after date drop 52685\n",
      "number of nans on the vios\n",
      "0\n",
      "             Outcome  Count\n",
      "0      Vacate Issued   3890\n",
      "1  Vacate Not Issued  48795\n",
      "Percent minority class: 0.07972128291833179\n",
      "\n",
      "Read open data\n",
      "Length of open data 4089\n",
      "\n",
      "Merge vio and open data\n",
      "56774\n",
      "rename BIN\n",
      "\n",
      "No vios\n",
      "48795\n",
      "Vios\n",
      "3890\n",
      "Open\n",
      "4089\n",
      "Non aggregated building class\n",
      "creating building class 2, non aggregated\n",
      "['1_2_FAMILY_DWELLINGS', 'CHILDREN_RELATED', 'CONDOMINIUMS', 'DEPARTMENT_STORES', 'DINERS_FRANCHISED_TYPE_STAND', 'EDUCATIONAL_STRUCTURES', 'ELEVATOR_APTS', 'FACTORY_INDUSTRIAL_BLDGS', 'FUNERAL_HOME', 'GARAGES_GAS_STATIONS', 'GOVERNMENT_INSTALLATIONS', 'HOSPITALS_HEALTH', 'HOTELS', 'MISCELLANEOUS', 'MISSION_HOUSE_NON_RESIDENTIAL', 'OFFICE_BUILDINGS', 'PIERS_DOCKS_BULKHEADS', 'PLACES_OF_PUBLIC_ASSEMBLY', 'RELIGIOUS', 'RESIDENCE_MULTI_USE', 'SHOPPING_CENTER_W_PARKING_FACILITY', 'SINGLE_MULT_RES_W_STORES_OFFICES', 'SPORTS_FACILITY', 'STORES_WITH_APARTMENTS_ABOVE', 'STORE_BUILDING', 'THEATRES', 'VACANT_LAND', 'WALK_UP_APTS', 'WAREHOUSES', 'nan']\n",
      "\n",
      "\n",
      "Insert hpd and hpd NA\n",
      "1000005\n",
      "sum of hpd 37705.0\n",
      "percent hpd 0.6641244231514426\n",
      "\n",
      "Insert year and building area\n",
      "\n",
      "Insert multple dwelling owner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DOBUSER\\anaconda3_8\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BIN', 'Ownership Profile', 'Owner Type'], dtype='object')\n",
      "len of df 56774\n",
      "number of MDs 21787.0\n",
      "\n",
      "Insert census data\n",
      "income null\n",
      "2709\n",
      "\n",
      "CREATE CATEGORICAL VARIABLES\n",
      "Create high rise category\n",
      "Create year built category\n",
      "Create Building area Category\n",
      "Create income category\n",
      "\n",
      "CREATE DUMMIES\n",
      "No previous job filings\n",
      "len dfj before drop 416583\n",
      "len dfj after drop 416583\n",
      "len of df 56774\n",
      "number of complaints on BINs with no previous job filings 18933.0\n",
      "percent of BINs with no history of job filings 0.3334801141367527\n",
      "Previous class 1 and deed trasactions dummies\n",
      "len of dt before drop 673447\n",
      "len of dt after drop 34243\n",
      "len before drop 316954\n",
      "len after drop 84677\n",
      "len of thebins 56774\n",
      "len of df 56774\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n",
      "24000\n",
      "26000\n",
      "28000\n",
      "30000\n",
      "32000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "40000\n",
      "42000\n",
      "44000\n",
      "46000\n",
      "48000\n",
      "50000\n",
      "52000\n",
      "54000\n",
      "56000\n",
      "Create previous vacate dummy\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n",
      "24000\n",
      "26000\n",
      "28000\n",
      "30000\n",
      "32000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "40000\n",
      "42000\n",
      "44000\n",
      "46000\n",
      "48000\n",
      "50000\n",
      "52000\n",
      "54000\n",
      "56000\n",
      "len of previous vacate dic 7130\n",
      "Create text dummies\n",
      "remove puncuation\n",
      "remove numbers\n",
      "create text dummies\n",
      "0\n",
      "6000\n",
      "12000\n",
      "18000\n",
      "24000\n",
      "30000\n",
      "36000\n",
      "42000\n",
      "48000\n",
      "54000\n",
      "Create year built, building area, median income, build class and owner type dummies\n",
      "Year built\n",
      "Building area\n",
      "Median income\n",
      "Building class\n",
      "Owner type\n",
      "Create community board dummies\n",
      "only keep cbs 10 or greater\n",
      "Shape of final df (56774, 151)\n",
      "\n",
      "SAMPLING\n",
      "Random upsample, training only\n",
      "create open complaint dataset and then train, validation and test datasets\n",
      "len of opendf 4089\n",
      "len of df2 52685\n",
      "{'Vacate Issued', 'Vacate Not Issued'}\n",
      "len of before drop open f 52685\n",
      "len of f after drop open 52685\n",
      "Get holdout data that is not upsampled\n",
      "Randomly sort df2\n",
      "get the imbalanced holdout and test\n",
      "df2hold 10537\n",
      "df2testImbal 10537\n",
      "Get the training then upsample training only\n",
      "Balance the class\n",
      "Size of majority class 29308\n",
      "Size of minority class 2303\n",
      "Size of minority class upsampled 29308\n",
      "Size of final dataset  58616\n",
      "Randomly sort df2bal\n",
      "df2trainBal 58616\n",
      "\n",
      "DROP FIELDS\n",
      "Training\n",
      "Training shape (58616, 124)\n",
      "Testing\n",
      "Testing shape (10537, 124)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from sklearn.utils import resample\n",
    "pd.set_option('chained_assignment', None)\n",
    "\n",
    "\n",
    "path = \"C:\\\\Users\\\\DOBUSER\\\\Documents\\\\RM Tim Case Study\\\\Illegal Conversions\\\\\"\n",
    "\n",
    "print (\"Read violations data\")\n",
    "start_date = \"1-1-2010\"\n",
    "print (\"Clean vio data\")\n",
    "dfa = pd.read_csv(path + \"RapidMiner 45_violations.csv\",encoding='cp1252') # encoding='cp1252'\n",
    "dfa = CleanVioData (dfa, start_date)\n",
    "\n",
    "print ()\n",
    "print (\"Read open data\")\n",
    "dfb = pd.read_csv(path + \"RapidMiner 45_open_complaints.csv\",encoding='cp1252' )\n",
    "dfb = CleanOpenData(dfb)\n",
    "\n",
    "print ()\n",
    "print (\"Merge vio and open data\")\n",
    "df = MergeVioOpen(dfa, dfb)\n",
    "\n",
    "print ()\n",
    "df = CountClassesAndClean(df)\n",
    "\n",
    "#print (\"Sort date ascending\")\n",
    "#df = DateSortAscending(df)\n",
    "\n",
    "print (\"Non aggregated building class\")\n",
    "df = CleanBuildClass(df)\n",
    "\n",
    "print ()\n",
    "#print (\"Aggregated building class\")\n",
    "#df = BuildingClassAggregate(df)\n",
    "\n",
    "\n",
    "#print ()\n",
    "#print (\"Insert class 1 and wwp vios\")\n",
    "#dv = pd.read_csv(path + \"Class1_issued.csv\", encoding='cp1252')\n",
    "#dw = pd.read_csv(path + \"RapidMiner wwp_vios.csv\", encoding='cp1252')\n",
    "#df = Class1WWPVios(df, dv, dw)\n",
    "\n",
    "#print (\"Cluster sample\")\n",
    "#df = ClusterSample(df)\n",
    "\n",
    "print ()\n",
    "print (\"Insert hpd and hpd NA\")\n",
    "hpd = pd.read_csv(path + \"Comprehensive 1- 317K HPD List.csv\", encoding='cp1252')\n",
    "df = HPDandHPDNA(df, hpd)\n",
    "\n",
    "print ()\n",
    "print (\"Insert year and building area\")\n",
    "dfb = pd.read_csv(path + \"Pluto Property Info by BIN.csv\", encoding='cp1252')\n",
    "dfb = dfb.rename(columns={'P_BIN': 'BIN', 'Year Built': 'YearBuilt', 'Building Area':'BuildingArea'})\n",
    "df = YearBuildArea(df, dfb)\n",
    "\n",
    "\n",
    "print ()\n",
    "print (\"Insert multple dwelling owner\")\n",
    "pathmd = \"C:\\\\Users\\\\DOBUSER\\\\Documents\\\\DATA\\\\Ownership\\\\\"\n",
    "dfm = pd.read_csv(pathmd + \"Multiple Dwelling Ownership Profile Single Mode ALL.csv\")\n",
    "df = MultipleDwellingOwner(df, dfm)\n",
    "\n",
    "print ()\n",
    "print (\"Insert census data\")\n",
    "dc = pd.read_csv(path + \"Census Income data.csv\")\n",
    "df = CensusData(df, dc)\n",
    "\n",
    "print ()\n",
    "print (\"CREATE CATEGORICAL VARIABLES\")\n",
    "print(\"Create high rise category\")  \n",
    "df[\"HighRise\"] = df['P_NUMBER_OF_STORIES'].apply(highrise_cat)\n",
    "\n",
    "print (\"Create year built category\")\n",
    "df[\"YearBuiltCat\"] = df['YearBuilt'].apply(YearBuilt)\n",
    "\n",
    "print (\"Create Building area Category\")\n",
    "df[\"BuildingAreaCat\"] = df['BuildingArea'].apply(BuildArea)\n",
    "\n",
    "print (\"Create income category\")\n",
    "df['MedianIncome'] = df['MedianIncome'].fillna(0)    \n",
    "df['MedianIncomeCategory'] = df['MedianIncome'].apply(income_cat)\n",
    "\n",
    "print ()\n",
    "print (\"CREATE DUMMIES\")\n",
    "print (\"No previous job filings\")\n",
    "dfj  = pd.read_csv(path + \"Job filling unique_bin.csv\",encoding='cp1252', low_memory=False)\n",
    "df = NoPreviousJobFiling(df, dfj)\n",
    "print (\"Previous class 1 and deed trasactions dummies\")\n",
    "dfc  = pd.read_csv(path + \"Class1_issued.csv\",encoding='cp1252', low_memory=False)\n",
    "dt = pd.read_csv(path + \"TRANSACTIONS 11 YEARS edited.csv\",encoding='cp1252', low_memory=False)\n",
    "df = PreviousClass1Transactions(df, dfc, dt)\n",
    "print (\"Create previous vacate dummy\")\n",
    "df = PreviousVacate(df)\n",
    "\n",
    "print (\"Create text dummies\")\n",
    "df = TextDummies(df)\n",
    "\n",
    "\n",
    "\n",
    "print (\"Create year built, building area, median income, build class and owner type dummies\")\n",
    "df = OtherDummies(df)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print (\"Create community board dummies\")\n",
    "df = CommBoardDummies(df)\n",
    "print (\"Shape of final df\", df.shape)\n",
    "#print (\"Create owner dummies\")\n",
    "#df = OwnerDummies(df)\n",
    "\n",
    "print ()\n",
    "print ('SAMPLING')\n",
    "#print (\"Random upsample, training and test\")\n",
    "#df2trainBal, df2testBal, df2hold = RandomUpsampleTrainingTest(df)\n",
    "print (\"Random upsample, training only\")\n",
    "df2trainBal, df2testImbal, df2hold, opendf = RandomUpsampleTraining(df)\n",
    "\n",
    "\n",
    "print ()\n",
    "print (\"DROP FIELDS\")\n",
    "print (\"Training\")\n",
    "df2trainBal = DropFields(df2trainBal)\n",
    "print (\"Training shape\",df2trainBal.shape )\n",
    "print (\"Testing\")\n",
    "#df2testBal = DropFields(df2testBal)\n",
    "df2testImbal = DropFields(df2testImbal)\n",
    "print (\"Testing shape\", df2testImbal.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path + \"Illegal Conversions Violations Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateReceived                    datetime64[ns]\n",
       "ComplaintCategory                        int64\n",
       "ComplaintCategoryDescription            object\n",
       "ComplaintNumber                          int64\n",
       "BIN                                     object\n",
       "                                     ...      \n",
       "CB_413                                   uint8\n",
       "CB_414                                   uint8\n",
       "CB_501                                   uint8\n",
       "CB_502                                   uint8\n",
       "CB_503                                   uint8\n",
       "Length: 151, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregated Building Class\")\n",
    "pd.set_option('chained_assignment', None)\n",
    "bldClass = sorted(list(set(df[\"BldgClass\"].tolist())))\n",
    "#print (bldClass)               \n",
    "bldclass = df[[\"BldgClass\", \"Outcome\"]]  \n",
    "bldclass = OutcomeConvert(bldclass)\n",
    "bldclass[\"Count\"] = 1\n",
    "bldclass = bldclass.groupby(['BldgClass']).sum()\n",
    "bldclass = bldclass.add_suffix('').reset_index()\n",
    "bldclass['Count'] = bldclass['Count'].astype(float)\n",
    "bldclass[\"Percent\"] = bldclass['Outcome']/bldclass['Count']\n",
    "#bldclass[\"Percent\"] = bldclass['SWOissued']/bldclass['Count'].sum()\n",
    "bldclass[\"Percent\"] = bldclass[\"Percent\"]*100\n",
    "#sumperc = bldclass[\"Percent\"].sum()\n",
    "#print (sumperc)\n",
    "bldclass = bldclass.sort_values(by = 'Outcome', ascending=False).reset_index(drop=True)\n",
    "bldclass[\"SWOPercentTotal\"] = (bldclass[\"Outcome\"]/float(bldclass[\"Outcome\"].sum()))*100\n",
    "print (bldclass)\n",
    "\n",
    "print ()\n",
    "yearbuilt = df[[\"Outcome\", \"YearBuiltCat\"]]\n",
    "yearbuilt = OutcomeConvert(yearbuilt)\n",
    "yearbuilt[\"Count\"] = 1\n",
    "yearbuilt = yearbuilt.groupby(['YearBuiltCat']).sum()\n",
    "yearbuilt = yearbuilt.add_suffix('').reset_index()\n",
    "yearbuilt['Count'] = yearbuilt['Count'].astype(float)\n",
    "print (yearbuilt)\n",
    "\n",
    "buildingarea = df[[\"Outcome\", \"BuildingAreaCat\"]]\n",
    "buildingarea = OutcomeConvert(buildingarea)\n",
    "buildingarea[\"Count\"] = 1\n",
    "buildingarea = buildingarea.groupby(['BuildingAreaCat']).sum()\n",
    "buildingarea = buildingarea.add_suffix('').reset_index()\n",
    "buildingarea['Count'] = buildingarea['Count'].astype(float)\n",
    "print (buildingarea)\n",
    "\n",
    "print ()\n",
    "highrise = df[[\"Outcome\", \"HighRise\"]]\n",
    "highrise = OutcomeConvert(highrise)\n",
    "highrise[\"Count\"] = 1\n",
    "highrise = highrise.groupby(['HighRise']).sum()\n",
    "highrise = highrise.add_suffix('').reset_index()\n",
    "highrise['Count'] = highrise['Count'].astype(float)\n",
    "print (highrise)\n",
    "\n",
    "print()\n",
    "medincome = df[[\"Outcome\", \"MedianIncomeCategory\"]]\n",
    "medincome = OutcomeConvert(medincome)\n",
    "medincome[\"Count\"] = 1\n",
    "medincome = medincome.groupby(['MedianIncomeCategory']).sum()\n",
    "medincome = medincome.add_suffix('').reset_index()\n",
    "medincome['Count'] = medincome['Count'].astype(float)\n",
    "print (medincome)\n",
    "\n",
    "\n",
    "print()\n",
    "owntype = df[[\"Outcome\", \"OwnerType\"]]\n",
    "owntype = OutcomeConvert(owntype)\n",
    "owntype[\"Count\"] = 1\n",
    "owntype = owntype.groupby(['OwnerType']).sum()\n",
    "owntype = owntype.add_suffix('').reset_index()\n",
    "owntype = owntype.sort_values(by = 'Outcome', ascending=False).reset_index(drop=True)\n",
    "print (\"Owner type\")\n",
    "print (owntype)\n",
    "\n",
    "\n",
    "print()\n",
    "comboard = df[[\"Outcome\", \"CommunityBoard\"]]\n",
    "comboard = OutcomeConvert(comboard)\n",
    "comboard[\"Count\"] = 1\n",
    "comboard = comboard.groupby(['CommunityBoard']).sum()\n",
    "comboard = comboard.add_suffix('').reset_index()\n",
    "#comboard['Count'] = comboard['Count'].astype(float)\n",
    "comboard = comboard.sort_values(by = 'Outcome', ascending=False).reset_index(drop=True)\n",
    "print (comboard[:20])\n",
    "\n",
    "\n",
    "job = df[[\"Outcome\", \"NoPreviousJob\"]]\n",
    "job = OutcomeConvert(job)\n",
    "job = job.groupby(['NoPreviousJob']).sum()\n",
    "job = job.add_suffix('').reset_index()\n",
    "\n",
    "print (job)\n",
    "print (job[\"Outcome\"]/job[\"Outcome\"].sum())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print ()\n",
    "print (\"Sum of SWO on CB\",comboard[\"SWOissued\"].sum() )\n",
    "print (\"Number of CBs\", len(set(comboard[\"CommunityBoard\"].tolist())))\n",
    "print (\"Sum of SWO on CB Top 10\", comboard[\"SWOissued\"][0:10].sum())\n",
    "print (\"Sum of SWO on CB Top 20\", comboard[\"SWOissued\"][0:20].sum())\n",
    "print (\"Sum of SWO on CB Top 30\", comboard[\"SWOissued\"][0:30].sum())\n",
    "\n",
    "\n",
    "print(\"Non-Aggregated Building Class\")\n",
    "pd.set_option('chained_assignment', None)\n",
    "bldClass = sorted(list(set(df[\"BldgClass2\"].tolist())))\n",
    "#print (bldClass)               \n",
    "bldclass = df[[\"BldgClass2\", \"Outcome\"]]  \n",
    "bldclass = OutcomeConvert(bldclass)\n",
    "bldclass[\"Count\"] = 1\n",
    "bldclass = bldclass.groupby(['BldgClass2']).sum()\n",
    "bldclass = bldclass.add_suffix('').reset_index()\n",
    "bldclass['Count'] = bldclass['Count'].astype(float)\n",
    "bldclass[\"Percent\"] = bldclass['Outcome']/bldclass['Count']\n",
    "#bldclass[\"Percent\"] = bldclass['SWOissued']/bldclass['Count'].sum()\n",
    "bldclass[\"Percent\"] = bldclass[\"Percent\"]*100\n",
    "#sumperc = bldclass[\"Percent\"].sum()\n",
    "#print (sumperc)\n",
    "bldclass = bldclass.sort_values(by = 'Outcome', ascending=False).reset_index(drop=True)\n",
    "bldclass[\"SWOPercentTotal\"] = (bldclass[\"Outcome\"]/float(bldclass[\"Outcome\"].sum()))*100\n",
    "#print (bldclass)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job[\"Outcome\"]/job[\"Outcome\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-23 09:53:08.665516\n",
      "\n",
      "Baseline Accuracy: Dummy majority score: 0.93\n",
      "IMPLEMENT SEVERAL MODELS\n",
      "Neural Networks\n",
      "Logistic Regression\n",
      "precision: 0.14959178298656833 recall: 0.7217280813214739 (C=0.001,default,na)\n",
      "precision: 0.15977653631284916 recall: 0.7268106734434562 (C=0.01,default,na)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DOBUSER\\anaconda3_8\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.15814341300722623 recall: 0.7229987293519695 (C=0.1,default,na)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DOBUSER\\anaconda3_8\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.15667311411992263 recall: 0.7204574332909784 (C=1,default,na)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DOBUSER\\anaconda3_8\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.15704070407040704 recall: 0.7255400254129606 (C=100,default,na)\n",
      "Gradient boosted trees\n",
      "precision: 0.10294659300184163 recall: 0.7102922490470139 (learn rate=0.0001,max depth=2,n_estim100)\n",
      "precision: 0.1043605181536216 recall: 0.7268106734434562 (learn rate=0.0001,max depth=3,n_estim100)\n",
      "precision: 0.10514305774969282 recall: 0.761118170266836 (learn rate=0.0001,max depth=4,n_estim100)\n",
      "precision: 0.12987918215613384 recall: 0.7102922490470139 (learn rate=0.0001,max depth=5,n_estim100)\n",
      "precision: 0.13201992753623187 recall: 0.7407878017789072 (learn rate=0.0001,max depth=6,n_estim100)\n",
      "precision: 0.10433847257081391 recall: 0.7395171537484116 (learn rate=0.001,max depth=2,n_estim100)\n",
      "precision: 0.1043605181536216 recall: 0.7268106734434562 (learn rate=0.001,max depth=3,n_estim100)\n",
      "precision: 0.14491017964071856 recall: 0.6149936467598475 (learn rate=0.001,max depth=4,n_estim100)\n",
      "precision: 0.1297071129707113 recall: 0.7090216010165185 (learn rate=0.001,max depth=5,n_estim100)\n",
      "precision: 0.1319900384876613 recall: 0.7407878017789072 (learn rate=0.001,max depth=6,n_estim100)\n",
      "precision: 0.1371841155234657 recall: 0.7242693773824651 (learn rate=0.01,max depth=2,n_estim100)\n",
      "precision: 0.14 recall: 0.7026683608640406 (learn rate=0.01,max depth=3,n_estim100)\n",
      "precision: 0.14388681260010677 recall: 0.6848792884371029 (learn rate=0.01,max depth=4,n_estim100)\n",
      "precision: 0.13981547196593327 recall: 0.7509529860228716 (learn rate=0.01,max depth=5,n_estim100)\n",
      "precision: 0.14134105161601543 recall: 0.7445997458703939 (learn rate=0.01,max depth=6,n_estim100)\n",
      "precision: 0.15713065755764305 recall: 0.7013977128335451 (learn rate=0.1,max depth=2,n_estim100)\n",
      "precision: 0.15874363327674024 recall: 0.7128335451080051 (learn rate=0.1,max depth=3,n_estim100)\n",
      "precision: 0.15654676258992806 recall: 0.6912325285895807 (learn rate=0.1,max depth=4,n_estim100)\n",
      "precision: 0.1588235294117647 recall: 0.6861499364675985 (learn rate=0.1,max depth=5,n_estim100)\n",
      "precision: 0.16024279210925646 recall: 0.6709021601016518 (learn rate=0.1,max depth=6,n_estim100)\n",
      "precision: 0.1578341013824885 recall: 0.6963151207115629 (learn rate=0.2,max depth=2,n_estim100)\n",
      "precision: 0.15804680728113263 recall: 0.6950444726810674 (learn rate=0.2,max depth=3,n_estim100)\n",
      "precision: 0.16008254716981132 recall: 0.6899618805590851 (learn rate=0.2,max depth=4,n_estim100)\n",
      "precision: 0.16226993865030676 recall: 0.6721728081321474 (learn rate=0.2,max depth=5,n_estim100)\n",
      "precision: 0.1627140974967062 recall: 0.627700127064803 (learn rate=0.2,max depth=6,n_estim100)\n",
      "precision: 0.1614835948644793 recall: 0.7191867852604829 (learn rate=0.3,max depth=2,n_estim100)\n",
      "precision: 0.15950029052876236 recall: 0.6975857687420585 (learn rate=0.3,max depth=3,n_estim100)\n",
      "precision: 0.15716395864106353 recall: 0.6759847522236341 (learn rate=0.3,max depth=4,n_estim100)\n",
      "precision: 0.1590765338393422 recall: 0.639135959339263 (learn rate=0.3,max depth=5,n_estim100)\n",
      "precision: 0.16252661462029808 recall: 0.5819567979669632 (learn rate=0.3,max depth=6,n_estim100)\n",
      "precision: 0.1601027397260274 recall: 0.7128335451080051 (learn rate=0.4,max depth=2,n_estim100)\n",
      "precision: 0.1586327782646801 recall: 0.6899618805590851 (learn rate=0.4,max depth=3,n_estim100)\n",
      "precision: 0.15768645357686453 recall: 0.6581956797966964 (learn rate=0.4,max depth=4,n_estim100)\n",
      "precision: 0.16220792414493734 recall: 0.6086404066073697 (learn rate=0.4,max depth=5,n_estim100)\n",
      "precision: 0.16924265842349304 recall: 0.5565438373570522 (learn rate=0.4,max depth=6,n_estim100)\n",
      "precision: 0.15856115107913668 recall: 0.7001270648030495 (learn rate=0.5,max depth=2,n_estim100)\n",
      "precision: 0.15795724465558195 recall: 0.6759847522236341 (learn rate=0.5,max depth=3,n_estim100)\n",
      "precision: 0.15738117721120554 recall: 0.6353240152477764 (learn rate=0.5,max depth=4,n_estim100)\n",
      "precision: 0.1587861679604799 recall: 0.5717916137229987 (learn rate=0.5,max depth=5,n_estim100)\n",
      "precision: 0.16909975669099755 recall: 0.5298602287166455 (learn rate=0.5,max depth=6,n_estim100)\n",
      "Random forests\n",
      "precision: 0.14956521739130435 recall: 0.3278271918678526 (Estimators=1,default,na)\n",
      "precision: 0.17133706965572457 recall: 0.2719186785260483 (Estimators=5,default,na)\n",
      "precision: 0.17668825161887142 recall: 0.24269377382465057 (Estimators=10,default,na)\n",
      "precision: 0.18262411347517732 recall: 0.2617534942820839 (Estimators=15,default,na)\n",
      "precision: 0.18796296296296297 recall: 0.2579415501905972 (Estimators=20,default,na)\n",
      "precision: 0.18538812785388128 recall: 0.2579415501905972 (Estimators=25,default,na)\n",
      "precision: 0.1842346471127406 recall: 0.2554002541296061 (Estimators=30,default,na)\n",
      "precision: 0.18886861313868614 recall: 0.2630241423125794 (Estimators=40,default,na)\n",
      "precision: 0.18676337262012693 recall: 0.2617534942820839 (Estimators=50,default,na)\n"
     ]
    }
   ],
   "source": [
    "print (str(datetime.now()) )\n",
    "print ()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "    \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "#create data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    \n",
    "model_selection = []\n",
    "    \n",
    "def AccuracyAndAUC(model, clf, X_test, y_test, a, b, c):\n",
    "    a = \"(\" + a + \",\"\n",
    "    b = b + \",\" \n",
    "    c = c +  \")\"\n",
    "    accuracy_param = a + b + c\n",
    "    clf_predicted = clf.predict(X_test)\n",
    "        \n",
    "    pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "    roc=roc_auc_score(y_test, pred_prob)\n",
    "    #print ('AUC:', round(roc, 2))\n",
    "\n",
    "        \n",
    "    #clf_predicted = clf.predict(X_test)\n",
    "    #confusion = confusion_matrix(y_test, clf_predicted)\n",
    "\n",
    "    #print('Classifier Confusion Matrix\\n', confusion)\n",
    "    print(\"precision:\", precision_score(y_test, clf_predicted), \"recall:\", recall_score(y_test, clf_predicted),accuracy_param )\n",
    "        \n",
    "    model_selection.append([model,accuracy_param, clf.score(X_train, y_train), accuracy_score(y_test, clf_predicted),\n",
    "                               precision_score(y_test, clf_predicted),recall_score(y_test, clf_predicted), \n",
    "                                f1_score(y_test, clf_predicted), roc])\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "def AccuracySVM(clf, X_test, y_test, a, b):\n",
    "    a = \"(\" + a + \",\"\n",
    "    b = b + \")\"\n",
    "    clf_predicted = clf.predict(X_test)\n",
    "    print ()\n",
    "    print (\"Accuracy on these parameters\", a, b)\n",
    "    print('Accuracy on test set: {:.2f}'.format(accuracy_score(y_test, clf_predicted)))\n",
    "    print('Precision: {:.2f}'.format(precision_score(y_test, clf_predicted)))\n",
    "    print('Recall: {:.2f}'.format(recall_score(y_test, clf_predicted)))\n",
    "    print('F1: {:.2f}'.format(f1_score(y_test, clf_predicted)))\n",
    "        \n",
    "    #pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "    #roc=roc_auc_score(y_test, pred_prob)\n",
    "    #print ('AUC:', round(roc, 2))\n",
    "        \n",
    "    y_score_gb = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_score_gb)\n",
    "    roc_auc_gb = auc(fpr_lr, tpr_lr)\n",
    "    print (\"AUC:\", round(roc_auc_gb, 2))\n",
    "\n",
    "\n",
    "    clf_predicted = clf.predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, clf_predicted)\n",
    "\n",
    "    print('Classifier Confusion Matrix\\n', confusion)\n",
    "        \n",
    "        \n",
    "def PlotROC(clf, X_test, y_test):\n",
    "    y_score_gb = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_score_gb)\n",
    "    roc_auc = auc(fpr_lr, tpr_lr)\n",
    "    #PLOT\n",
    "    plt.figure()\n",
    "    plt.xlim([-0.01, 1.00])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.plot(fpr_lr, tpr_lr, lw=3, label='ROC curve (area = {:0.2f})'.format(roc_auc))\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "    plt.legend(loc='lower right', fontsize=13)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "        \n",
    "    return plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#RANDOM\n",
    "y_train = df2trainBal[\"Outcome\"]\n",
    "X_train = df2trainBal.drop('Outcome', 1)\n",
    "#print (\"Reducing indep vars using feature importance\")\n",
    "#X_train = X_train[importantfeatures]\n",
    "\n",
    "\n",
    "#y_test = df2testBal[\"Outcome\"]\n",
    "#X_test = df2testBal.drop('Outcome', 1)\n",
    "\n",
    "y_test = df2testImbal[\"Outcome\"]\n",
    "X_test = df2testImbal.drop('Outcome', 1)\n",
    "#X_test = X_test[importantfeatures]\n",
    "\n",
    "\n",
    "#test baseline using dummy classifier\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "    \n",
    "print (\"Baseline Accuracy: Dummy majority score:\", round(dummy_majority.score(X_test, y_test), 2) )\n",
    "#print ()\n",
    "\n",
    "print (\"IMPLEMENT SEVERAL MODELS\" )\n",
    "\n",
    "#hidden_layers = [(10), (20), (30), (40), (50), (60), (70), (80), (90), (100) ]\n",
    "\n",
    "hidden_layers = [(100), (100,100),  (100,100,100)  ]\n",
    "\n",
    "#hidden_layers = [(20),  (60),(100),\n",
    "#                 (20,20), (60,60), (100,100),\n",
    "#               (20,20,20),(60,60,60), (100,100,100)  ]\n",
    "\n",
    "\n",
    "#hidden_layers = [(100)]\n",
    "\n",
    "#hidden_layers = [(10), (20), (30), (40), (50), (60), (70), (80), (90), (100), (150), (200),\n",
    "#                 (10,10), (20,20), (30,30), (40,40), \n",
    "#                 (50,50), (60,60), (70,70),\n",
    "#                 (10,10,10), (20,20,20), (30,30,30), \n",
    "#                 (40,40,40), (50,50,50), (60,60,60), (70,70,70)]\n",
    "\n",
    "#hidden_layers = [(20),  (40),(60), (80), (100), (120), (140), (160), (180), (200),\n",
    "#                 (20,20), (40,40), (60,60), (80,80), (100,100), (120,120), (140,140), (160,160), (180, 180), (200,200),\n",
    "#                (20,20,20), (40,40,40), (60,60,60), (80,80,80), (100,100,100), (120,120,120), (140,140,140), (160,160,160),\n",
    "#                (180,180, 180), (200, 200, 200), (250, 250, 250),(300, 300, 300)  ]\n",
    "\n",
    "iterations = 1000\n",
    "\n",
    "#NEURAL NETWORKS\n",
    "print (\"Neural Networks\")\n",
    "\n",
    "\"\"\"\n",
    "for layer in hidden_layers:\n",
    "    clf = MLPClassifier(hidden_layer_sizes = layer, max_iter=iterations) \n",
    "    clf.fit(X_train, y_train)\n",
    "    modelstr = \"Neural Networks\"\n",
    "    a = str(layer)\n",
    "    b = str(iterations)\n",
    "    print (modelstr, a)\n",
    "    AccuracyAndAUC(modelstr, clf, X_test, y_test, a, b)\n",
    "\n",
    "\n",
    "#print (\"Naive Bayes\")\n",
    "#clf = GaussianNB().fit(X_train, y_train)\n",
    "#modelstr = \"Naive Bayes\"\n",
    "#a = \"default\"\n",
    "#b = \"default\"\n",
    "#AccuracyAndAUC(modelstr, clf, X_test, y_test, a, b)\n",
    "\"\"\"\n",
    "\n",
    "print (\"Logistic Regression\")\n",
    "this_C = [0.001, 0.01, 0.1, 1, 100]\n",
    "modelstr = \"Logistic Regression\"\n",
    "\n",
    "for the_c in this_C:\n",
    "    a = \"C=\" + str(the_c)\n",
    "    b = \"default\"\n",
    "    c = \"na\"\n",
    "    clf = LogisticRegression(C=the_c).fit(X_train, y_train)\n",
    "    AccuracyAndAUC(modelstr, clf, X_test, y_test, a, b, c)\n",
    "\n",
    "\n",
    "print (\"Gradient boosted trees\")\n",
    "learn_rate = [0.0001, 0.001, 0.01, 0.1,0.2,0.3,0.4, 0.5]\n",
    "#learn_rate = [ 0.01, 0.1,0.2,0.3,0.4, 0.5]\n",
    "mdepth = [2, 3, 4, 5, 6]\n",
    "#est = [100, 200, 300]\n",
    "est = [100]\n",
    "\n",
    "\n",
    "for learn in learn_rate:\n",
    "    for depth in mdepth:\n",
    "        for e in est:\n",
    "            clf = GradientBoostingClassifier(learning_rate = learn, max_depth = depth,n_estimators=e, random_state = 0)\n",
    "            clf.fit(X_train, y_train)\n",
    "            modelstr = \"GBT\"\n",
    "            a = \"learn rate=\" + str(learn)\n",
    "            b = \"max depth=\" + str(depth)\n",
    "            c = \"n_estim\" + str(e)\n",
    "            AccuracyAndAUC(modelstr, clf, X_test, y_test, a, b, c)\n",
    "\n",
    "\n",
    "print (\"Random forests\")\n",
    "\n",
    "estimators = [1, 5,10,15,20, 25, 30, 40, 50]\n",
    "\n",
    "for est in estimators:\n",
    "    clf = RandomForestClassifier(n_estimators = est,random_state=0).fit(X_train, y_train)\n",
    "    modelstr = \"Random Forests\"\n",
    "    a = \"Estimators=\" + str(est)\n",
    "    b = \"default\"\n",
    "    c = \"na\"\n",
    "    AccuracyAndAUC(modelstr, clf, X_test, y_test, a, b, c)\n",
    "    \n",
    "\n",
    "#clf = RandomForestClassifier(max_features = 8, random_state = 0)\n",
    "    \n",
    "df_model = pd.DataFrame(model_selection, columns=('Model','Parameters', 'Training Score', 'Test Score', 'Precision', 'Recall', 'F1', 'AUC'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Training Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.3,max depth=2,n_estim100)</td>\n",
       "      <td>0.713065</td>\n",
       "      <td>0.700104</td>\n",
       "      <td>0.161484</td>\n",
       "      <td>0.719187</td>\n",
       "      <td>0.263747</td>\n",
       "      <td>0.768951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>(C=0.01,default,na)</td>\n",
       "      <td>0.701259</td>\n",
       "      <td>0.694125</td>\n",
       "      <td>0.159777</td>\n",
       "      <td>0.726811</td>\n",
       "      <td>0.261965</td>\n",
       "      <td>0.772806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.4,max depth=2,n_estim100)</td>\n",
       "      <td>0.718968</td>\n",
       "      <td>0.699250</td>\n",
       "      <td>0.160103</td>\n",
       "      <td>0.712834</td>\n",
       "      <td>0.261478</td>\n",
       "      <td>0.769154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.2,max depth=5,n_estim100)</td>\n",
       "      <td>0.777075</td>\n",
       "      <td>0.716333</td>\n",
       "      <td>0.162270</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>0.261428</td>\n",
       "      <td>0.761040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.2,max depth=4,n_estim100)</td>\n",
       "      <td>0.751928</td>\n",
       "      <td>0.706463</td>\n",
       "      <td>0.160083</td>\n",
       "      <td>0.689962</td>\n",
       "      <td>0.259871</td>\n",
       "      <td>0.763691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.1,max depth=3,n_estim100)</td>\n",
       "      <td>0.711956</td>\n",
       "      <td>0.696403</td>\n",
       "      <td>0.158744</td>\n",
       "      <td>0.712834</td>\n",
       "      <td>0.259662</td>\n",
       "      <td>0.770812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.3,max depth=3,n_estim100)</td>\n",
       "      <td>0.734509</td>\n",
       "      <td>0.702857</td>\n",
       "      <td>0.159500</td>\n",
       "      <td>0.697586</td>\n",
       "      <td>0.259636</td>\n",
       "      <td>0.767762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.4,max depth=6,n_estim100)</td>\n",
       "      <td>0.866999</td>\n",
       "      <td>0.762836</td>\n",
       "      <td>0.169243</td>\n",
       "      <td>0.556544</td>\n",
       "      <td>0.259556</td>\n",
       "      <td>0.735540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>(C=0.1,default,na)</td>\n",
       "      <td>0.704159</td>\n",
       "      <td>0.691848</td>\n",
       "      <td>0.158143</td>\n",
       "      <td>0.722999</td>\n",
       "      <td>0.259521</td>\n",
       "      <td>0.771991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.1,max depth=6,n_estim100)</td>\n",
       "      <td>0.777791</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.160243</td>\n",
       "      <td>0.670902</td>\n",
       "      <td>0.258697</td>\n",
       "      <td>0.765204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.5,max depth=2,n_estim100)</td>\n",
       "      <td>0.722277</td>\n",
       "      <td>0.700104</td>\n",
       "      <td>0.158561</td>\n",
       "      <td>0.700127</td>\n",
       "      <td>0.258564</td>\n",
       "      <td>0.768428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.2,max depth=6,n_estim100)</td>\n",
       "      <td>0.810512</td>\n",
       "      <td>0.730948</td>\n",
       "      <td>0.162714</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.258436</td>\n",
       "      <td>0.754545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>(C=100,default,na)</td>\n",
       "      <td>0.705388</td>\n",
       "      <td>0.688621</td>\n",
       "      <td>0.157041</td>\n",
       "      <td>0.725540</td>\n",
       "      <td>0.258196</td>\n",
       "      <td>0.768510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.4,max depth=3,n_estim100)</td>\n",
       "      <td>0.749573</td>\n",
       "      <td>0.703521</td>\n",
       "      <td>0.158633</td>\n",
       "      <td>0.689962</td>\n",
       "      <td>0.257957</td>\n",
       "      <td>0.765584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.1,max depth=5,n_estim100)</td>\n",
       "      <td>0.753719</td>\n",
       "      <td>0.705134</td>\n",
       "      <td>0.158824</td>\n",
       "      <td>0.686150</td>\n",
       "      <td>0.257941</td>\n",
       "      <td>0.768258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.2,max depth=3,n_estim100)</td>\n",
       "      <td>0.726440</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.158047</td>\n",
       "      <td>0.695044</td>\n",
       "      <td>0.257533</td>\n",
       "      <td>0.769881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>(C=1,default,na)</td>\n",
       "      <td>0.705524</td>\n",
       "      <td>0.689475</td>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.720457</td>\n",
       "      <td>0.257376</td>\n",
       "      <td>0.769545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.2,max depth=2,n_estim100)</td>\n",
       "      <td>0.705115</td>\n",
       "      <td>0.699820</td>\n",
       "      <td>0.157834</td>\n",
       "      <td>0.696315</td>\n",
       "      <td>0.257337</td>\n",
       "      <td>0.768279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.1,max depth=2,n_estim100)</td>\n",
       "      <td>0.697472</td>\n",
       "      <td>0.696688</td>\n",
       "      <td>0.157131</td>\n",
       "      <td>0.701398</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.766906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.5,max depth=6,n_estim100)</td>\n",
       "      <td>0.879845</td>\n",
       "      <td>0.770428</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.529860</td>\n",
       "      <td>0.256379</td>\n",
       "      <td>0.719769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.4,max depth=5,n_estim100)</td>\n",
       "      <td>0.820083</td>\n",
       "      <td>0.735978</td>\n",
       "      <td>0.162208</td>\n",
       "      <td>0.608640</td>\n",
       "      <td>0.256150</td>\n",
       "      <td>0.742820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.5,max depth=3,n_estim100)</td>\n",
       "      <td>0.752525</td>\n",
       "      <td>0.706653</td>\n",
       "      <td>0.157957</td>\n",
       "      <td>0.675985</td>\n",
       "      <td>0.256077</td>\n",
       "      <td>0.758075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.1,max depth=4,n_estim100)</td>\n",
       "      <td>0.727634</td>\n",
       "      <td>0.698776</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>0.691233</td>\n",
       "      <td>0.255279</td>\n",
       "      <td>0.769689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.3,max depth=4,n_estim100)</td>\n",
       "      <td>0.765610</td>\n",
       "      <td>0.705039</td>\n",
       "      <td>0.157164</td>\n",
       "      <td>0.675985</td>\n",
       "      <td>0.255034</td>\n",
       "      <td>0.760614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.3,max depth=5,n_estim100)</td>\n",
       "      <td>0.799492</td>\n",
       "      <td>0.720698</td>\n",
       "      <td>0.159077</td>\n",
       "      <td>0.639136</td>\n",
       "      <td>0.254748</td>\n",
       "      <td>0.747520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.4,max depth=4,n_estim100)</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>0.711872</td>\n",
       "      <td>0.157686</td>\n",
       "      <td>0.658196</td>\n",
       "      <td>0.254420</td>\n",
       "      <td>0.752054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.3,max depth=6,n_estim100)</td>\n",
       "      <td>0.842534</td>\n",
       "      <td>0.744804</td>\n",
       "      <td>0.162527</td>\n",
       "      <td>0.581957</td>\n",
       "      <td>0.254092</td>\n",
       "      <td>0.742435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.5,max depth=4,n_estim100)</td>\n",
       "      <td>0.796335</td>\n",
       "      <td>0.718706</td>\n",
       "      <td>0.157381</td>\n",
       "      <td>0.635324</td>\n",
       "      <td>0.252270</td>\n",
       "      <td>0.746208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.5,max depth=5,n_estim100)</td>\n",
       "      <td>0.836273</td>\n",
       "      <td>0.741767</td>\n",
       "      <td>0.158786</td>\n",
       "      <td>0.571792</td>\n",
       "      <td>0.248550</td>\n",
       "      <td>0.733280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>(C=0.001,default,na)</td>\n",
       "      <td>0.687304</td>\n",
       "      <td>0.672772</td>\n",
       "      <td>0.149592</td>\n",
       "      <td>0.721728</td>\n",
       "      <td>0.247818</td>\n",
       "      <td>0.755781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.01,max depth=4,n_estim100)</td>\n",
       "      <td>0.683090</td>\n",
       "      <td>0.672108</td>\n",
       "      <td>0.143887</td>\n",
       "      <td>0.684879</td>\n",
       "      <td>0.237812</td>\n",
       "      <td>0.753414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.01,max depth=6,n_estim100)</td>\n",
       "      <td>0.706104</td>\n",
       "      <td>0.643067</td>\n",
       "      <td>0.141341</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.237584</td>\n",
       "      <td>0.757583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.01,max depth=5,n_estim100)</td>\n",
       "      <td>0.696107</td>\n",
       "      <td>0.636329</td>\n",
       "      <td>0.139815</td>\n",
       "      <td>0.750953</td>\n",
       "      <td>0.235740</td>\n",
       "      <td>0.757421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.001,max depth=4,n_estim100)</td>\n",
       "      <td>0.660724</td>\n",
       "      <td>0.700199</td>\n",
       "      <td>0.144910</td>\n",
       "      <td>0.614994</td>\n",
       "      <td>0.234553</td>\n",
       "      <td>0.709165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.01,max depth=3,n_estim100)</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>0.655405</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.702668</td>\n",
       "      <td>0.233481</td>\n",
       "      <td>0.751597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.01,max depth=2,n_estim100)</td>\n",
       "      <td>0.663948</td>\n",
       "      <td>0.639176</td>\n",
       "      <td>0.137184</td>\n",
       "      <td>0.724269</td>\n",
       "      <td>0.230676</td>\n",
       "      <td>0.737820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.0001,max depth=6,n_estim100)</td>\n",
       "      <td>0.675208</td>\n",
       "      <td>0.616874</td>\n",
       "      <td>0.132020</td>\n",
       "      <td>0.740788</td>\n",
       "      <td>0.224101</td>\n",
       "      <td>0.730033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.001,max depth=6,n_estim100)</td>\n",
       "      <td>0.675976</td>\n",
       "      <td>0.616779</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>0.740788</td>\n",
       "      <td>0.224058</td>\n",
       "      <td>0.738059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=40,default,na)</td>\n",
       "      <td>0.959397</td>\n",
       "      <td>0.860587</td>\n",
       "      <td>0.188869</td>\n",
       "      <td>0.263024</td>\n",
       "      <td>0.219862</td>\n",
       "      <td>0.700254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.0001,max depth=5,n_estim100)</td>\n",
       "      <td>0.660349</td>\n",
       "      <td>0.622948</td>\n",
       "      <td>0.129879</td>\n",
       "      <td>0.710292</td>\n",
       "      <td>0.219603</td>\n",
       "      <td>0.725337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.001,max depth=5,n_estim100)</td>\n",
       "      <td>0.660570</td>\n",
       "      <td>0.622948</td>\n",
       "      <td>0.129707</td>\n",
       "      <td>0.709022</td>\n",
       "      <td>0.219297</td>\n",
       "      <td>0.733056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=50,default,na)</td>\n",
       "      <td>0.959397</td>\n",
       "      <td>0.859732</td>\n",
       "      <td>0.186763</td>\n",
       "      <td>0.261753</td>\n",
       "      <td>0.217989</td>\n",
       "      <td>0.703223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=20,default,na)</td>\n",
       "      <td>0.959329</td>\n",
       "      <td>0.861346</td>\n",
       "      <td>0.187963</td>\n",
       "      <td>0.257942</td>\n",
       "      <td>0.217461</td>\n",
       "      <td>0.682377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=25,default,na)</td>\n",
       "      <td>0.959380</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>0.185388</td>\n",
       "      <td>0.257942</td>\n",
       "      <td>0.215728</td>\n",
       "      <td>0.689895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=15,default,na)</td>\n",
       "      <td>0.959260</td>\n",
       "      <td>0.857360</td>\n",
       "      <td>0.182624</td>\n",
       "      <td>0.261753</td>\n",
       "      <td>0.215144</td>\n",
       "      <td>0.673882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=30,default,na)</td>\n",
       "      <td>0.959380</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.214058</td>\n",
       "      <td>0.695884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=5,default,na)</td>\n",
       "      <td>0.957520</td>\n",
       "      <td>0.847395</td>\n",
       "      <td>0.171337</td>\n",
       "      <td>0.271919</td>\n",
       "      <td>0.210216</td>\n",
       "      <td>0.638162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=1,default,na)</td>\n",
       "      <td>0.943070</td>\n",
       "      <td>0.810572</td>\n",
       "      <td>0.149565</td>\n",
       "      <td>0.327827</td>\n",
       "      <td>0.205414</td>\n",
       "      <td>0.591743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>(Estimators=10,default,na)</td>\n",
       "      <td>0.959175</td>\n",
       "      <td>0.858973</td>\n",
       "      <td>0.176688</td>\n",
       "      <td>0.242694</td>\n",
       "      <td>0.204497</td>\n",
       "      <td>0.659090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.0001,max depth=4,n_estim100)</td>\n",
       "      <td>0.619080</td>\n",
       "      <td>0.498339</td>\n",
       "      <td>0.105143</td>\n",
       "      <td>0.761118</td>\n",
       "      <td>0.184762</td>\n",
       "      <td>0.688727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.001,max depth=2,n_estim100)</td>\n",
       "      <td>0.612990</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.104338</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.182875</td>\n",
       "      <td>0.690746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.0001,max depth=3,n_estim100)</td>\n",
       "      <td>0.608673</td>\n",
       "      <td>0.513714</td>\n",
       "      <td>0.104361</td>\n",
       "      <td>0.726811</td>\n",
       "      <td>0.182514</td>\n",
       "      <td>0.666690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.001,max depth=3,n_estim100)</td>\n",
       "      <td>0.608673</td>\n",
       "      <td>0.513714</td>\n",
       "      <td>0.104361</td>\n",
       "      <td>0.726811</td>\n",
       "      <td>0.182514</td>\n",
       "      <td>0.705713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>GBT</td>\n",
       "      <td>(learn rate=0.0001,max depth=2,n_estim100)</td>\n",
       "      <td>0.597243</td>\n",
       "      <td>0.516086</td>\n",
       "      <td>0.102947</td>\n",
       "      <td>0.710292</td>\n",
       "      <td>0.179829</td>\n",
       "      <td>0.641967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model                                  Parameters  \\\n",
       "0                   GBT     (learn rate=0.3,max depth=2,n_estim100)   \n",
       "1   Logistic Regression                         (C=0.01,default,na)   \n",
       "2                   GBT     (learn rate=0.4,max depth=2,n_estim100)   \n",
       "3                   GBT     (learn rate=0.2,max depth=5,n_estim100)   \n",
       "4                   GBT     (learn rate=0.2,max depth=4,n_estim100)   \n",
       "5                   GBT     (learn rate=0.1,max depth=3,n_estim100)   \n",
       "6                   GBT     (learn rate=0.3,max depth=3,n_estim100)   \n",
       "7                   GBT     (learn rate=0.4,max depth=6,n_estim100)   \n",
       "8   Logistic Regression                          (C=0.1,default,na)   \n",
       "9                   GBT     (learn rate=0.1,max depth=6,n_estim100)   \n",
       "10                  GBT     (learn rate=0.5,max depth=2,n_estim100)   \n",
       "11                  GBT     (learn rate=0.2,max depth=6,n_estim100)   \n",
       "12  Logistic Regression                          (C=100,default,na)   \n",
       "13                  GBT     (learn rate=0.4,max depth=3,n_estim100)   \n",
       "14                  GBT     (learn rate=0.1,max depth=5,n_estim100)   \n",
       "15                  GBT     (learn rate=0.2,max depth=3,n_estim100)   \n",
       "16  Logistic Regression                            (C=1,default,na)   \n",
       "17                  GBT     (learn rate=0.2,max depth=2,n_estim100)   \n",
       "18                  GBT     (learn rate=0.1,max depth=2,n_estim100)   \n",
       "19                  GBT     (learn rate=0.5,max depth=6,n_estim100)   \n",
       "20                  GBT     (learn rate=0.4,max depth=5,n_estim100)   \n",
       "21                  GBT     (learn rate=0.5,max depth=3,n_estim100)   \n",
       "22                  GBT     (learn rate=0.1,max depth=4,n_estim100)   \n",
       "23                  GBT     (learn rate=0.3,max depth=4,n_estim100)   \n",
       "24                  GBT     (learn rate=0.3,max depth=5,n_estim100)   \n",
       "25                  GBT     (learn rate=0.4,max depth=4,n_estim100)   \n",
       "26                  GBT     (learn rate=0.3,max depth=6,n_estim100)   \n",
       "27                  GBT     (learn rate=0.5,max depth=4,n_estim100)   \n",
       "28                  GBT     (learn rate=0.5,max depth=5,n_estim100)   \n",
       "29  Logistic Regression                        (C=0.001,default,na)   \n",
       "30                  GBT    (learn rate=0.01,max depth=4,n_estim100)   \n",
       "31                  GBT    (learn rate=0.01,max depth=6,n_estim100)   \n",
       "32                  GBT    (learn rate=0.01,max depth=5,n_estim100)   \n",
       "33                  GBT   (learn rate=0.001,max depth=4,n_estim100)   \n",
       "34                  GBT    (learn rate=0.01,max depth=3,n_estim100)   \n",
       "35                  GBT    (learn rate=0.01,max depth=2,n_estim100)   \n",
       "36                  GBT  (learn rate=0.0001,max depth=6,n_estim100)   \n",
       "37                  GBT   (learn rate=0.001,max depth=6,n_estim100)   \n",
       "38       Random Forests                  (Estimators=40,default,na)   \n",
       "39                  GBT  (learn rate=0.0001,max depth=5,n_estim100)   \n",
       "40                  GBT   (learn rate=0.001,max depth=5,n_estim100)   \n",
       "41       Random Forests                  (Estimators=50,default,na)   \n",
       "42       Random Forests                  (Estimators=20,default,na)   \n",
       "43       Random Forests                  (Estimators=25,default,na)   \n",
       "44       Random Forests                  (Estimators=15,default,na)   \n",
       "45       Random Forests                  (Estimators=30,default,na)   \n",
       "46       Random Forests                   (Estimators=5,default,na)   \n",
       "47       Random Forests                   (Estimators=1,default,na)   \n",
       "48       Random Forests                  (Estimators=10,default,na)   \n",
       "49                  GBT  (learn rate=0.0001,max depth=4,n_estim100)   \n",
       "50                  GBT   (learn rate=0.001,max depth=2,n_estim100)   \n",
       "51                  GBT  (learn rate=0.0001,max depth=3,n_estim100)   \n",
       "52                  GBT   (learn rate=0.001,max depth=3,n_estim100)   \n",
       "53                  GBT  (learn rate=0.0001,max depth=2,n_estim100)   \n",
       "\n",
       "    Training Score  Test Score  Precision    Recall        F1       AUC  \n",
       "0         0.713065    0.700104   0.161484  0.719187  0.263747  0.768951  \n",
       "1         0.701259    0.694125   0.159777  0.726811  0.261965  0.772806  \n",
       "2         0.718968    0.699250   0.160103  0.712834  0.261478  0.769154  \n",
       "3         0.777075    0.716333   0.162270  0.672173  0.261428  0.761040  \n",
       "4         0.751928    0.706463   0.160083  0.689962  0.259871  0.763691  \n",
       "5         0.711956    0.696403   0.158744  0.712834  0.259662  0.770812  \n",
       "6         0.734509    0.702857   0.159500  0.697586  0.259636  0.767762  \n",
       "7         0.866999    0.762836   0.169243  0.556544  0.259556  0.735540  \n",
       "8         0.704159    0.691848   0.158143  0.722999  0.259521  0.771991  \n",
       "9         0.777791    0.712821   0.160243  0.670902  0.258697  0.765204  \n",
       "10        0.722277    0.700104   0.158561  0.700127  0.258564  0.768428  \n",
       "11        0.810512    0.730948   0.162714  0.627700  0.258436  0.754545  \n",
       "12        0.705388    0.688621   0.157041  0.725540  0.258196  0.768510  \n",
       "13        0.749573    0.703521   0.158633  0.689962  0.257957  0.765584  \n",
       "14        0.753719    0.705134   0.158824  0.686150  0.257941  0.768258  \n",
       "15        0.726440    0.700674   0.158047  0.695044  0.257533  0.769881  \n",
       "16        0.705524    0.689475   0.156673  0.720457  0.257376  0.769545  \n",
       "17        0.705115    0.699820   0.157834  0.696315  0.257337  0.768279  \n",
       "18        0.697472    0.696688   0.157131  0.701398  0.256744  0.766906  \n",
       "19        0.879845    0.770428   0.169100  0.529860  0.256379  0.719769  \n",
       "20        0.820083    0.735978   0.162208  0.608640  0.256150  0.742820  \n",
       "21        0.752525    0.706653   0.157957  0.675985  0.256077  0.758075  \n",
       "22        0.727634    0.698776   0.156547  0.691233  0.255279  0.769689  \n",
       "23        0.765610    0.705039   0.157164  0.675985  0.255034  0.760614  \n",
       "24        0.799492    0.720698   0.159077  0.639136  0.254748  0.747520  \n",
       "25        0.782227    0.711872   0.157686  0.658196  0.254420  0.752054  \n",
       "26        0.842534    0.744804   0.162527  0.581957  0.254092  0.742435  \n",
       "27        0.796335    0.718706   0.157381  0.635324  0.252270  0.746208  \n",
       "28        0.836273    0.741767   0.158786  0.571792  0.248550  0.733280  \n",
       "29        0.687304    0.672772   0.149592  0.721728  0.247818  0.755781  \n",
       "30        0.683090    0.672108   0.143887  0.684879  0.237812  0.753414  \n",
       "31        0.706104    0.643067   0.141341  0.744600  0.237584  0.757583  \n",
       "32        0.696107    0.636329   0.139815  0.750953  0.235740  0.757421  \n",
       "33        0.660724    0.700199   0.144910  0.614994  0.234553  0.709165  \n",
       "34        0.676300    0.655405   0.140000  0.702668  0.233481  0.751597  \n",
       "35        0.663948    0.639176   0.137184  0.724269  0.230676  0.737820  \n",
       "36        0.675208    0.616874   0.132020  0.740788  0.224101  0.730033  \n",
       "37        0.675976    0.616779   0.131990  0.740788  0.224058  0.738059  \n",
       "38        0.959397    0.860587   0.188869  0.263024  0.219862  0.700254  \n",
       "39        0.660349    0.622948   0.129879  0.710292  0.219603  0.725337  \n",
       "40        0.660570    0.622948   0.129707  0.709022  0.219297  0.733056  \n",
       "41        0.959397    0.859732   0.186763  0.261753  0.217989  0.703223  \n",
       "42        0.959329    0.861346   0.187963  0.257942  0.217461  0.682377  \n",
       "43        0.959380    0.859922   0.185388  0.257942  0.215728  0.689895  \n",
       "44        0.959260    0.857360   0.182624  0.261753  0.215144  0.673882  \n",
       "45        0.959380    0.859922   0.184235  0.255400  0.214058  0.695884  \n",
       "46        0.957520    0.847395   0.171337  0.271919  0.210216  0.638162  \n",
       "47        0.943070    0.810572   0.149565  0.327827  0.205414  0.591743  \n",
       "48        0.959175    0.858973   0.176688  0.242694  0.204497  0.659090  \n",
       "49        0.619080    0.498339   0.105143  0.761118  0.184762  0.688727  \n",
       "50        0.612990    0.506406   0.104338  0.739517  0.182875  0.690746  \n",
       "51        0.608673    0.513714   0.104361  0.726811  0.182514  0.666690  \n",
       "52        0.608673    0.513714   0.104361  0.726811  0.182514  0.705713  \n",
       "53        0.597243    0.516086   0.102947  0.710292  0.179829  0.641967  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_model.sort_values(by = 'F1', ascending=False).reset_index(drop=True)\n",
    "#df_model.to_csv(path + \"GBT Results.csv\", index=False)\n",
    "#df_model2 = df_model[df_model[\"Recall\"] > 0.7]\n",
    "#df_model2 = df_model2[df_model2[\"F1\"] > 0.25]\n",
    "\n",
    "#df_model2[\"DiffTrainTest\"] = df_model2[\"Training Score\"] - df_model2[\"Test Score\"]\n",
    "#df_model2 = df_model2.sort_values(by = 'DiffTrainTest', ascending=True).reset_index(drop=True)\n",
    "\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv(path + \"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_modelhold = df_modelhold.sort_values(by = 'Recall', ascending=False).reset_index(drop=True)\n",
    "#df_modelhold[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS OF HOLDOUT USING BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (58616, 123)\n",
      "Holdout shape (10537, 123)\n",
      "Confusion Matrix\n",
      "[[6893 2844]\n",
      " [ 256  544]]\n",
      "Calculate predictions using best model\n",
      "create predicted outcome\n",
      "DONE\n",
      "GradientBoostingClassifier(learning_rate=0.4, max_depth=2, random_state=0)\n",
      "Size of final test data (holdout data): 10537\n",
      "True positive: 800\n",
      "Correct Predictions: 544\n",
      "Percent correctly predicted: 0.68\n",
      "False negatives: 256\n",
      "Percent false negatives: 0.32\n",
      "False positives: 2844\n",
      "Percent false positives: 0.27\n"
     ]
    }
   ],
   "source": [
    "X_train2 = X_train.copy()\n",
    "y_train2 = y_train.copy()\n",
    "\n",
    "X_hold2 = df2hold.copy()\n",
    "X_hold2 = DropFields(X_hold2)\n",
    "print (\"Training shape\",X_train2.shape )\n",
    "#X_hold_raw = X_hold.copy()\n",
    "#X_hold2 = X_hold.copy()\n",
    "#X_hold = df_hold.copy()\n",
    "y_hold2 = X_hold2[\"Outcome\"]\n",
    "X_hold2 = X_hold2.drop('Outcome', 1)\n",
    "#print (\"Reducing number of features\")\n",
    "#X_hold2 = X_hold2[importantfeatures]\n",
    "print (\"Holdout shape\",X_hold2.shape )\n",
    "\n",
    "#fit the best model\n",
    "#neural networks\n",
    "#clf = MLPClassifier(hidden_layer_sizes = ([100, 100, 100]), max_iter=1000) \n",
    "#naive bayes\n",
    "#clf = GaussianNB()\n",
    "#random forests\n",
    "#clf = RandomForestClassifier(n_estimators = 30,random_state=0)\n",
    "#Gradient Boosted Trees\n",
    "clf = GradientBoostingClassifier(learning_rate = 0.4, max_depth = 2, random_state = 0)\n",
    "#clf = GradientBoostingClassifier(learning_rate = 0.001, max_depth = 6, random_state = 0)\n",
    "\n",
    "#logistic regression\n",
    "#clf = LogisticRegression(C=1)\n",
    "\n",
    "clf.fit(X_train2, y_train2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "clf_predicted = clf.predict(X_hold2)\n",
    "confusion = confusion_matrix(y_hold2, clf_predicted)\n",
    "print (\"Confusion Matrix\")\n",
    "print (confusion)\n",
    "\n",
    "\n",
    "print (\"Calculate predictions using best model\")\n",
    "result  =   clf.predict_proba(X_hold2)\n",
    "#assign positive class probability predictions\n",
    "    \n",
    "X_hold2[\"PredProb\"] = result[:,1]\n",
    "\n",
    "X_hold2 = pd.concat([X_hold2, y_hold2], axis=1)\n",
    "\n",
    "print (\"create predicted outcome\")\n",
    "X_hold2[\"PredOutcome\"] = 0\n",
    "\n",
    "X_hold2[\"Index\"] = X_hold2.index\n",
    "X_hold2 = X_hold2.reset_index(drop=True)\n",
    "\n",
    "for i in range(0, len(X_hold2)):\n",
    "    if X_hold2[\"PredProb\"][i] >= 0.5:\n",
    "        X_hold2[\"PredOutcome\"][i] = 1\n",
    "        \n",
    "print (\"DONE\")\n",
    "\n",
    "print (clf)\n",
    "print (\"Size of final test data (holdout data):\", len(X_hold2))\n",
    "print (\"True positive:\", sum(confusion[1]) )\n",
    "print (\"Correct Predictions:\",confusion[1][1] )\n",
    "print (\"Percent correctly predicted:\",round(float(confusion[1][1])/sum(confusion[1]), 2) )\n",
    "print (\"False negatives:\", confusion[1][0])\n",
    "print (\"Percent false negatives:\",  round(confusion[1][0]/sum(confusion[1]),2) )\n",
    "print (\"False positives:\", confusion[0][1] )\n",
    "print (\"Percent false positives:\", round(float(confusion[0][1])/len(X_hold2), 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Holdout shape (10537, 123)\n",
    "Confusion Matrix\n",
    "[[6798 2946]\n",
    " [ 228  565]]\n",
    "Calculate predictions using best model\n",
    "create predicted outcome\n",
    "DONE\n",
    "GradientBoostingClassifier(learning_rate=0.4, max_depth=2, random_state=0)\n",
    "Size of final test data (holdout data): 10537\n",
    "True positive: 793\n",
    "Correct Predictions: 565\n",
    "Percent correctly predicted: 0.71\n",
    "False negatives: 228\n",
    "Percent false negatives: 0.29\n",
    "False positives: 2946\n",
    "Percent false positives: 0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET FEATURE IMPORTANCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train2.columns, clf.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "    #print (feature, importance)\n",
    "    \n",
    "x = pd.DataFrame(list(feats.items()),columns = ['Feature','Importance']) \n",
    "\n",
    "x = x.sort_values(by = 'Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#for i in range(0, len(x)):\n",
    "#    print (x[\"Feature\"][i], x[\"Importance\"][i].round(3))\n",
    "\n",
    "x2 = x[x[\"Importance\"] > 0.01]\n",
    "\n",
    "importantfeatures = x2[\"Feature\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt_basement</td>\n",
       "      <td>0.122239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt_fdny</td>\n",
       "      <td>0.100288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt_cellar</td>\n",
       "      <td>0.100145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA_&lt;3000</td>\n",
       "      <td>0.093625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CB_412</td>\n",
       "      <td>0.077323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HPDNA</td>\n",
       "      <td>0.059873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BA_3000_6000</td>\n",
       "      <td>0.052832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CB_413</td>\n",
       "      <td>0.047394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HighRise</td>\n",
       "      <td>0.023797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CB_311</td>\n",
       "      <td>0.017794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BA_&gt;20000</td>\n",
       "      <td>0.016997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BC_1_2_FAMILY_DWELLINGS</td>\n",
       "      <td>0.014359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>YB_1916_1937</td>\n",
       "      <td>0.013364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CB_409</td>\n",
       "      <td>0.013237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CB_410</td>\n",
       "      <td>0.013009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>OT_IndividualOwner</td>\n",
       "      <td>0.012680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PreviousVacate</td>\n",
       "      <td>0.011148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MedIncome_High</td>\n",
       "      <td>0.010424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HPD</td>\n",
       "      <td>0.010083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BC_WALK_UP_APTS</td>\n",
       "      <td>0.009828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature  Importance\n",
       "0              txt_basement    0.122239\n",
       "1                  txt_fdny    0.100288\n",
       "2                txt_cellar    0.100145\n",
       "3                  BA_<3000    0.093625\n",
       "4                    CB_412    0.077323\n",
       "5                     HPDNA    0.059873\n",
       "6              BA_3000_6000    0.052832\n",
       "7                    CB_413    0.047394\n",
       "8                  HighRise    0.023797\n",
       "9                    CB_311    0.017794\n",
       "10                BA_>20000    0.016997\n",
       "11  BC_1_2_FAMILY_DWELLINGS    0.014359\n",
       "12             YB_1916_1937    0.013364\n",
       "13                   CB_409    0.013237\n",
       "14                   CB_410    0.013009\n",
       "15       OT_IndividualOwner    0.012680\n",
       "16           PreviousVacate    0.011148\n",
       "17           MedIncome_High    0.010424\n",
       "18                      HPD    0.010083\n",
       "19          BC_WALK_UP_APTS    0.009828"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['txt_cellar',\n",
       " 'txt_basement',\n",
       " 'txt_fdny',\n",
       " 'BA_<3000',\n",
       " 'CB_412',\n",
       " 'BA_3000_6000',\n",
       " 'HPDNA',\n",
       " 'CB_413',\n",
       " 'OT_HeadOfficer',\n",
       " 'HighRise',\n",
       " 'txt_occupancy',\n",
       " 'CB_410',\n",
       " 'CB_409',\n",
       " 'MedIncome_High',\n",
       " 'BA_>20000',\n",
       " 'CB_311']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (len(importantfeatures))\n",
    "importantfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def plot_feature_importances(clf, feature_names):\n",
    "    c_features = len(feature_names)\n",
    "    plt.barh(range(c_features), clf.feature_importances_)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature name\")\n",
    "    plt.yticks(numpy.arange(c_features), feature_names)\n",
    "    \n",
    "plot_feature_importances(clf, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTIONS USING BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateReceived                    datetime64[ns]\n",
       "ComplaintCategory                        int64\n",
       "ComplaintCategoryDescription            object\n",
       "ComplaintNumber                          int64\n",
       "BIN                                     object\n",
       "CommunityBoard                          object\n",
       "BoroughName                             object\n",
       "SourceofComplaintDescription            object\n",
       "Latitude                               float64\n",
       "Longitude                              float64\n",
       "P_NUMBER_OF_STORIES                      int64\n",
       "DOFBuildingClassification               object\n",
       "BBL_SPATIAL_KEY                         object\n",
       "FinanceOwnerName                        object\n",
       "HouseNumber                             object\n",
       "StreetName                              object\n",
       "RegistrationID                         float64\n",
       "Outcome                                 object\n",
       "ComplaintReason                         object\n",
       "BldgClass                               object\n",
       "HPD                                    float64\n",
       "HPDNA                                  float64\n",
       "YearBuilt                              float64\n",
       "BuildingArea                           float64\n",
       "Owner                                   object\n",
       "OwnerType                               object\n",
       "MD                                     float64\n",
       "MedianIncome                           float64\n",
       "HighRise                                 int64\n",
       "YearBuiltCat                            object\n",
       "BuildingAreaCat                         object\n",
       "MedianIncomeCategory                    object\n",
       "NoPreviousJob                          float64\n",
       "PreviousClass1                           int64\n",
       "PreviousSale                             int64\n",
       "PreviousVacate                         float64\n",
       "txt_fdny                                 int64\n",
       "txt_occupancy                            int64\n",
       "txt_cellar                               int64\n",
       "txt_attic                                int64\n",
       "txt_basement                             int64\n",
       "YB_1900_1915                             uint8\n",
       "YB_1916_1937                             uint8\n",
       "YB_1938_1967                             uint8\n",
       "YB_1968_2007                             uint8\n",
       "YB_2008_2021                             uint8\n",
       "YB_pre_1900                              uint8\n",
       "BA_12000_20000                           uint8\n",
       "BA_3000_6000                             uint8\n",
       "BA_6000_9000                             uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opendf.dtypes[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4089, 123)\n",
      "create predicted outcome\n",
      "Length of opendf2 4089\n",
      "Number of positive predicted outcomes 2108\n"
     ]
    }
   ],
   "source": [
    "opendf2 = opendf.copy()\n",
    "opendf2[\"Index\"] = opendf2.index\n",
    "opendf2 = DropFields(opendf2)\n",
    "opendf2 = opendf2.drop('Outcome', 1)\n",
    "\n",
    "#opendf2 = opendf2[importantfeatures]\n",
    "\n",
    "print (opendf2.shape)\n",
    "\n",
    "result  =   clf.predict_proba(opendf2)\n",
    "#assign positive class probability predictions\n",
    "    \n",
    "opendf2[\"PredProb\"] = result[:,1]\n",
    "\n",
    "\n",
    "#X_test_im = pd.concat([X_test_im, y_test_im], axis=1)\n",
    "\n",
    "print (\"create predicted outcome\")\n",
    "opendf2[\"PredOutcome\"] = 0\n",
    "\n",
    "#X_test_im[\"Index\"] = X_test_im.index\n",
    "opendf2 = opendf2.reset_index(drop=True)\n",
    "\n",
    "for i in range(0, len(opendf2)):\n",
    "    if opendf2[\"PredProb\"][i] >= 0.5:\n",
    "        opendf2[\"PredOutcome\"][i] = 1\n",
    "        \n",
    "print (\"Length of opendf2\", len(opendf2))\n",
    "print (\"Number of positive predicted outcomes\",opendf2[\"PredOutcome\"].sum() )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge to get categorical data\n",
      "opendf shape (4089, 151)\n",
      "opedf2 shape (4089, 125)\n",
      "output shape (4089, 2)\n",
      "opendfoutput shape (4089, 41)\n",
      "output2 shape (4089, 43)\n"
     ]
    }
   ],
   "source": [
    "print (\"Merge to get categorical data\")\n",
    "opendfcolumns = opendf.columns[:41]\n",
    "\n",
    "print (\"opendf shape\", opendf.shape)\n",
    "print (\"opedf2 shape\", opendf2.shape)\n",
    "\n",
    "output = opendf2[[\"PredProb\",\"PredOutcome\"]]\n",
    "opendfoutput = opendf[opendfcolumns]\n",
    "\n",
    "print (\"output shape\", output.shape)\n",
    "print (\"opendfoutput shape\", opendfoutput.shape)\n",
    "\n",
    "\n",
    "\n",
    "output = output.reset_index(drop=True)\n",
    "opendfoutput = opendfoutput.reset_index(drop=True)\n",
    "\n",
    "output2  = pd.concat([output, opendfoutput], axis=1)\n",
    "\n",
    "print (\"output2 shape\", output2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DateReceived', 'ComplaintCategory', 'ComplaintCategoryDescription',\n",
       "       'ComplaintNumber', 'BIN', 'CommunityBoard', 'BoroughName',\n",
       "       'SourceofComplaintDescription', 'Latitude', 'Longitude',\n",
       "       'P_NUMBER_OF_STORIES', 'DOFBuildingClassification', 'BBL_SPATIAL_KEY',\n",
       "       'FinanceOwnerName', 'HouseNumber', 'StreetName', 'RegistrationID',\n",
       "       'Outcome', 'ComplaintReason', 'BldgClass', 'HPD', 'HPDNA', 'YearBuilt',\n",
       "       'BuildingArea', 'Owner', 'OwnerType', 'MD', 'MedianIncome', 'HighRise',\n",
       "       'YearBuiltCat', 'BuildingAreaCat', 'MedianIncomeCategory',\n",
       "       'NoPreviousJob', 'PreviousClass1', 'PreviousSale', 'PreviousVacate',\n",
       "       'txt_fdny', 'txt_occupancy', 'txt_cellar', 'txt_attic', 'txt_basement'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opendfcolumns = opendf.columns[:41]\n",
    "opendfcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_csv(path + \"scoring on open data 2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
